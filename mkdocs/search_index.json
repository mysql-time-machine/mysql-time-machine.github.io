{
    "docs": [
        {
            "location": "/", 
            "text": "Overview\n\n\nMySQL Time Machine is a collection of services and tools for creating, processing and storing streams of MySQL data changes. Its main components are presented bellow.\n\n\n\nReplicator\n\n\nReplicates data changes from MySQL binlog to HBase or Kafka. In case of HBase it preserves the previous data versions. HBase version is intended for auditing of historical data. In addition can maintain special daily-changes tables which are convenient for fast and cheap imports from HBase to Hive. Kafka version is intended for processing data change streams in real time.\n\n\nThe components of the replicator are presented at the diagram bellow:\n\n\n\n\nThe numbers on the diagram mark the main tasks that the replicator does in order to reliably decode, decorate and deliver data change events to the designated target store:\n\n\n\n\n\n\nIt uses binlog client (OpenReplicator) which is decoding binary stream to a stream of java objects that can be further processed by the replicator pipeline.\n\n\n\n\n\n\nIt keeps track of schema changes that are happening and makes sure that the schema that is used to decorate events always corresponds to the position of the event in the binlog. This is important since if we were to just query the MySQL slave information_schema we do not have guarantee that the schema matches the schema of the event in the binlog. The way this is implemented is by having a shadow schema which only contains the schema and not the data. Whenever we have a DDL statement in the binlog we apply that statement to the shadow schema. We call this schema \nActive Schema\n because its state always corresponds to the\nposition of the event that was last processed, so it is always correct. The active schema is stored on a separate MySQL instance because that is a safe alternative to implementing a MySQL parser because MySQL always knows how to parse MySQL so there is no possibility of errors due to corner cases in syntax.\n\n\n\n\n\n\nIt decorates events with complete schema information even the information that\nis not present in binlog TABLE_MAP event (like if the number is unsigned). This is possible because we do not use TABLE_MAP event but instead we use the active schema from point 2.\n\n\n\n\n\n\nIt applies the schema-decorated events to the designated target. In case of HBase the applier is parallelized so ingestion rate can be very high - everything that can be pulled from MySQL can be written without replication delay.\n\n\n\n\n\n\nIt maintains safe check points in zookeeper which enable the failover mechanisms for high-availability setups, both for MySQL server failover and replicator instance failover.\n\n\n\n\n\n\nBinlog Flusher\n\n\nIn case we want to have the initial copy of the database in Kafka or HBase, before we start to replicate changes, we can achieve that by stoping the MySQL slave, flushing the database to the binlog and turning MySQL slave on again. This is the purpose of Binlog Flusher tool. It Flushes MySQL database tables to the binlog in order to have the initial snapshot of the database in the binlog.\n\n\nHBase Snapshotter\n\n\nIn case of replication to HBase, we can use the fact that HBase is temporal database which supports versioning. By default we keep 1000 versions. This means that for every field in MySQL table, on the HBase side we have all versions (with appropriate timestamps) since the replication has started. However, if we want to see how a given table looked at specific point in time, we can scan the HBase table and filter the timestamps, get the versions that correspond to desired point in time and store it in a Hive table. This is the purpose of HBaseSnapshotter.\n\n\nHBaseSnapshotter is a Spark application that takes a snapshot of a HBase table at a given point in time and stores it to a Hive table. This can not be done just by using HBase storage engine for Hive since it only works with the latest version. Spark does not have such limitations so it was used to make the snapshotter.\n\n\nValidator\n\n\nValidates the replicator correctness.", 
            "title": "MySQL Time Machine"
        }, 
        {
            "location": "/#overview", 
            "text": "MySQL Time Machine is a collection of services and tools for creating, processing and storing streams of MySQL data changes. Its main components are presented bellow.", 
            "title": "Overview"
        }, 
        {
            "location": "/#replicator", 
            "text": "Replicates data changes from MySQL binlog to HBase or Kafka. In case of HBase it preserves the previous data versions. HBase version is intended for auditing of historical data. In addition can maintain special daily-changes tables which are convenient for fast and cheap imports from HBase to Hive. Kafka version is intended for processing data change streams in real time.  The components of the replicator are presented at the diagram bellow:   The numbers on the diagram mark the main tasks that the replicator does in order to reliably decode, decorate and deliver data change events to the designated target store:    It uses binlog client (OpenReplicator) which is decoding binary stream to a stream of java objects that can be further processed by the replicator pipeline.    It keeps track of schema changes that are happening and makes sure that the schema that is used to decorate events always corresponds to the position of the event in the binlog. This is important since if we were to just query the MySQL slave information_schema we do not have guarantee that the schema matches the schema of the event in the binlog. The way this is implemented is by having a shadow schema which only contains the schema and not the data. Whenever we have a DDL statement in the binlog we apply that statement to the shadow schema. We call this schema  Active Schema  because its state always corresponds to the\nposition of the event that was last processed, so it is always correct. The active schema is stored on a separate MySQL instance because that is a safe alternative to implementing a MySQL parser because MySQL always knows how to parse MySQL so there is no possibility of errors due to corner cases in syntax.    It decorates events with complete schema information even the information that\nis not present in binlog TABLE_MAP event (like if the number is unsigned). This is possible because we do not use TABLE_MAP event but instead we use the active schema from point 2.    It applies the schema-decorated events to the designated target. In case of HBase the applier is parallelized so ingestion rate can be very high - everything that can be pulled from MySQL can be written without replication delay.    It maintains safe check points in zookeeper which enable the failover mechanisms for high-availability setups, both for MySQL server failover and replicator instance failover.", 
            "title": "Replicator"
        }, 
        {
            "location": "/#binlog_flusher", 
            "text": "In case we want to have the initial copy of the database in Kafka or HBase, before we start to replicate changes, we can achieve that by stoping the MySQL slave, flushing the database to the binlog and turning MySQL slave on again. This is the purpose of Binlog Flusher tool. It Flushes MySQL database tables to the binlog in order to have the initial snapshot of the database in the binlog.", 
            "title": "Binlog Flusher"
        }, 
        {
            "location": "/#hbase_snapshotter", 
            "text": "In case of replication to HBase, we can use the fact that HBase is temporal database which supports versioning. By default we keep 1000 versions. This means that for every field in MySQL table, on the HBase side we have all versions (with appropriate timestamps) since the replication has started. However, if we want to see how a given table looked at specific point in time, we can scan the HBase table and filter the timestamps, get the versions that correspond to desired point in time and store it in a Hive table. This is the purpose of HBaseSnapshotter.  HBaseSnapshotter is a Spark application that takes a snapshot of a HBase table at a given point in time and stores it to a Hive table. This can not be done just by using HBase storage engine for Hive since it only works with the latest version. Spark does not have such limitations so it was used to make the snapshotter.", 
            "title": "HBase Snapshotter"
        }, 
        {
            "location": "/#validator", 
            "text": "Validates the replicator correctness.", 
            "title": "Validator"
        }, 
        {
            "location": "/binlog_flusher/", 
            "text": "Overview\n\n\nBinlog Flusher is a python script which flushes MySQL database tables to the binlog in order to have the initial snapshot of the database in the binlog.\n\n\nFlushing Database to the Binlog\n\n\nWARNING: you should NEVER run binlog-flusher on the MySQL master. Binlog flusher renames all tables during the blackhole copy and if the program does not finish successfully, the database can be left in an inconsistent state and in worst case you will need to reclone the database. The probability of this is low, but still, if it happens, you do NOT want this to happen on MySQL master.\n\n\nAssuming you adhere to the above warning, you can flush the contents of a database to the binlog with:\n\n\npython data-flusher.py --host $host\n                       [--mycnf $mycnf] \\\n                       [--db $db] [--table $table] \\\n                       [--stop-slave/--no-stop-slave] [--start-slave/--no-start-slave] \\\n                       [--skip $skip]\n\n\n\n\n\nWhere parameters are:\n\n\n\n\n--host\n: host name of the MySQL slave which databases need to be flushed to the binlog\n\n\n--mycnf\n: filename that contains the admin privileges used for the blackhole_copy of initial snapshot. Defaults to ~/my.cnf\n\n\n--db\n: comma separated list of databases to copy. Leave blank for all databases\n\n\n--table\n: comma separated list of tables to copy. Leave blank for all tables\n\n\n--stop-slave/--no-stop-slave\n: stop the replication thread whilst flushing the to the binlog. default=True\n\n\n--start-slave/--no-start-slave\n: restart the replication thread after finishing the flushing to the binlog. default=False\n\n\n--skip\n: separated list of schemas to skip (not to flush in the binlog)\n\n\n\n\nand the configuration file with admin credentials (\n--mycnf\n parameter above) has the following structure:\n\n\n[client]\nuser=admin\npassword=admin\n\n\n\n\nAfter the data has been flushed to the binlog, the replication is stopped by default (see default values for parameters above). You can start the replication with:\n\n\nmysql\n start slave;\n\n\n\n\nIn case binlog flusher didn\nt exit gracefully and the database has been left in an inconsistent state, you can run db-recovery.py to recover the database.\n\n\npython db-recovery.py --host $host \\\n                      --hashfile $hashfile \\\n                      [--mycnf .my.cnf] \\\n                      [--db $db] [--table $table] \\\n                      [--stop-slave/--no-stop-slave] \\\n                      [--start-slave/--no-start-slave] \\\n                      [--skip $skip]\n\n\n\n\nWhere \n--hashfile\n parameter contains the name of a rollback file, generated during the flush procedure, that contains the mappings from the backup table names to original table names.\n\n\n_BKTB_1, $tablename1$\n_BKTB_2, $tablename2$\n....", 
            "title": "Binlog Flusher"
        }, 
        {
            "location": "/binlog_flusher/#overview", 
            "text": "Binlog Flusher is a python script which flushes MySQL database tables to the binlog in order to have the initial snapshot of the database in the binlog.", 
            "title": "Overview"
        }, 
        {
            "location": "/binlog_flusher/#flushing_database_to_the_binlog", 
            "text": "WARNING: you should NEVER run binlog-flusher on the MySQL master. Binlog flusher renames all tables during the blackhole copy and if the program does not finish successfully, the database can be left in an inconsistent state and in worst case you will need to reclone the database. The probability of this is low, but still, if it happens, you do NOT want this to happen on MySQL master.  Assuming you adhere to the above warning, you can flush the contents of a database to the binlog with:  python data-flusher.py --host $host\n                       [--mycnf $mycnf] \\\n                       [--db $db] [--table $table] \\\n                       [--stop-slave/--no-stop-slave] [--start-slave/--no-start-slave] \\\n                       [--skip $skip]  Where parameters are:   --host : host name of the MySQL slave which databases need to be flushed to the binlog  --mycnf : filename that contains the admin privileges used for the blackhole_copy of initial snapshot. Defaults to ~/my.cnf  --db : comma separated list of databases to copy. Leave blank for all databases  --table : comma separated list of tables to copy. Leave blank for all tables  --stop-slave/--no-stop-slave : stop the replication thread whilst flushing the to the binlog. default=True  --start-slave/--no-start-slave : restart the replication thread after finishing the flushing to the binlog. default=False  --skip : separated list of schemas to skip (not to flush in the binlog)   and the configuration file with admin credentials ( --mycnf  parameter above) has the following structure:  [client]\nuser=admin\npassword=admin  After the data has been flushed to the binlog, the replication is stopped by default (see default values for parameters above). You can start the replication with:  mysql  start slave;  In case binlog flusher didn t exit gracefully and the database has been left in an inconsistent state, you can run db-recovery.py to recover the database.  python db-recovery.py --host $host \\\n                      --hashfile $hashfile \\\n                      [--mycnf .my.cnf] \\\n                      [--db $db] [--table $table] \\\n                      [--stop-slave/--no-stop-slave] \\\n                      [--start-slave/--no-start-slave] \\\n                      [--skip $skip]  Where  --hashfile  parameter contains the name of a rollback file, generated during the flush procedure, that contains the mappings from the backup table names to original table names.  _BKTB_1, $tablename1$\n_BKTB_2, $tablename2$\n....", 
            "title": "Flushing Database to the Binlog"
        }, 
        {
            "location": "/active_schema/", 
            "text": "Overview:\n\n\nActive schema is an empty copy of the database we want to replicate. It contains all the tables from the original database but the tables are empty. The point of this is to have a schema copy and to be able to update this copy independent of the main database because we need to have access to the schema that corresponds to a given position in the binlog (which can be different than the current schema of the database we are replicating).\n\n\nInitializing the Active Schema\n\n\nCreate new schema named \ntestdb_active_schema\n.\n\n\nCREATE DATABASE testdb_active_schema;\n\n\n\n\ndump the schema:\n\n\nmysqldump --host=localhost --user=test_user --password=test_pass --no-data --single-transaction testdb \n schema_dump.sql\n\n\n\n\nreplace \ntestdb.\n with \ntestdb_active_schema.\n in schema_dump.sql, for example in vim:\n\n\n%s/testdb\\./testdb_active_schema\\./g\n\n\n\n\nthen import the schema to testdb_active_schema:\n\n\nmysql --host=localhost --user=meta_user --password=meta_pass \n schema_dump.sql", 
            "title": "Active Schema"
        }, 
        {
            "location": "/active_schema/#overview", 
            "text": "Active schema is an empty copy of the database we want to replicate. It contains all the tables from the original database but the tables are empty. The point of this is to have a schema copy and to be able to update this copy independent of the main database because we need to have access to the schema that corresponds to a given position in the binlog (which can be different than the current schema of the database we are replicating).", 
            "title": "Overview:"
        }, 
        {
            "location": "/active_schema/#initializing_the_active_schema", 
            "text": "Create new schema named  testdb_active_schema .  CREATE DATABASE testdb_active_schema;  dump the schema:  mysqldump --host=localhost --user=test_user --password=test_pass --no-data --single-transaction testdb   schema_dump.sql  replace  testdb.  with  testdb_active_schema.  in schema_dump.sql, for example in vim:  %s/testdb\\./testdb_active_schema\\./g  then import the schema to testdb_active_schema:  mysql --host=localhost --user=meta_user --password=meta_pass   schema_dump.sql", 
            "title": "Initializing the Active Schema"
        }, 
        {
            "location": "/replicator/", 
            "text": "Overview\n\n\nReplicator reads the binary log events, augments them with schema information and writes the augmented stream to the chosen output. The following replication targets are supported: STDOUT, Kafka and HBase.\n\n\n\n\nBefore running the replicator, \nactive schema\n needs to be initialized.\n\n\nIn addition, if you want to have the initial copy of the data (and not only the changes), you need to \nflush\n the contents of the database to the binlog.\n\n\nOnce that is done, you can proceed with configuring the replicator for the chosen replication target.\n\n\nStdout\n\n\nThis option is usefull for troubleshouting and debuging. In the command line parameters we specify applier as stdout, schema which needs to be replicated, starting binlog filename and path to th config file.\n\n\njava -jar mysql-replicator.jar \\\n    --applier STDOUT \\\n    --schema $schema \\\n    --binlog-filename $binlog-filename \\\n    --config-path $config-path\n\n\n\n\nWhere the minimal configuration file is:\n\n\nreplication_schema:\n    name:      'testdb'\n    username:  'test_user'\n    password:  'test_pass'\n    host_pool: ['localhost']\n\nmetadata_store:\n    username: 'meta_user'\n    password: 'meta_pass'\n    host:     'localhost'\n    database: 'testdb_active_schema'\n    file:\n        path: '/path/on/disk'\n\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        console:\n            timeZone: UTC\n            output: stdout\n\n\n\n\nWhere \ntestdb\n is a schema on local mysql instance that is going to be monitored for changes and \ntestdb_active_schema\n is a empty copy of \ntestdb\n. By empty copy we mean a copy of the schema containing tables with no data. This copy can be created by mysqldump tool. An example is given in \nSetup Active Schema\n section.\n\n\nExample: assuming you have localhost database \ntestdb\n and have initialized active schema \ntestdb_active_schema\n:\n\n\nCreate a test table:\n\n\nCREATE TABLE `sometable` (\n  `pk_part_1` varchar(5) NOT NULL DEFAULT '',\n  `pk_part_2` int(11) NOT NULL DEFAULT '0',\n  `randomInt` int(11) DEFAULT NULL,\n  `randomVarchar` varchar(32) DEFAULT NULL,\n  PRIMARY KEY (`pk_part_1`,`pk_part_2`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n\n\n\n\nThen run the replicator with:\n\n\njava -jar mysql-replicator.jar \\\n    --applier STDOUT \\\n    --schema testdb \\\n    --binlog-filename $starting-binlog-file-path \\\n    --config-path $config-file-path\n\n\n\n\nIf you execute following sql in mysql shell:\n\n\ninsert into\n        sometable\n            (pk_part_1, pk_part_2, randomInt, randomVarchar)\n        values\n            ('yOeTX','3371509','911440','jfOZXWuNyJVfOzpjbsoc')\n\n\n\n\nYou will see the following output (here it is given with jq pretty print, but in STDOUT it is compressed json) from the replicator:\n\n\n{\n  \neventV4Header\n: {\n    \ntimestamp\n: 1494843484000008,\n    \neventType\n: 23,\n    \nserverId\n: 1,\n    \neventLength\n: 65,\n    \nnextPosition\n: 510,\n    \nflags\n: 0,\n    \ntimestampOfReceipt\n: 1494843484220\n  },\n  \nbinlogFileName\n: \nmysql-bin.000001\n,\n  \nrowBinlogEventOrdinal\n: 1,\n  \ntableName\n: \nsometable\n,\n  \nprimaryKeyColumns\n: [\n    \npk_part_1\n,\n    \npk_part_2\n\n  ],\n  \nrowUUID\n: \nd36af99b-52be-4700-86d0-8c503b50658b\n,\n  \nrowBinlogPositionID\n: \nmysql-bin.000001:00000000000000000445:00000000000000000001\n,\n  \neventColumns\n: {\n    \npk_part_1\n: {\n      \ntype\n: \nvarchar(5)\n,\n      \nvalue\n: \nyOeTX\n\n    },\n    \nrandomInt\n: {\n      \ntype\n: \nint(11)\n,\n      \nvalue\n: \n911440\n\n    },\n    \npk_part_2\n: {\n      \ntype\n: \nint(11)\n,\n      \nvalue\n: \n3371509\n\n    },\n    \nrandomVarchar\n: {\n      \ntype\n: \nvarchar(32)\n,\n      \nvalue\n: \njfOZXWuNyJVfOzpjbsoc\n\n    }\n  },\n  \neventType\n: \nINSERT\n\n}\n\n\n\n\nKafka\n\n\nYou need to have a kafka topic specified in the config file.\n\n\nreplication_schema:\n    name:      'replicated_schema_name'\n    username:  'user'\n    password:  'pass'\n    host_pool: ['host_1', ..., 'host_n']\nmetadata_store:\n    username: 'user'\n    password: 'pass'\n    host:     'active_schema_host'\n    database: 'active_schema_database'\n    # The following are options for storing replicator metadata,\n    # only one should be used (zookeeper or file)\n    file:\n        path: '/path/on/disk'\nkafka:\n    broker: \nkafka-broker-1:port,...,kafka-broken-N:port\n\n    topic:  topic_name\n    # tables to replicate to kafka, can be either a list of tables,\n    # or an exlcussion filter\n    tables: [\ntable_1\n, ..., \ntable_N\n]\n    excludetables: [\nexlude_pattern_1\n,..., \nexclude_pattern_N\n]\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        # The following are options for metrics reporters,\n        # only one should be used (graphite or console)\n        graphite:\n            namespace: 'graphite.namespace.prefix'\n            url: 'graphite_host[:\ngraphite_port (default is 3002)\n]'\n        console:\n            timeZone: UTC\n            output: stdout\n\n\n\n\nThen you start the replicator:\n\n\njava -jar mysql-replicator.jar \\\n    --applier kafka \\\n    --schema $schema \\\n    --binlog-filename $binlog-filename \\\n    --config-path $config-path\n\n\n\n\nHBase\n\n\nFirst step is seting up the config file, which needs to conaint the HBase zookeeper quorum\n\n\nreplication_schema:\n    name:      'replicated_schema_name'\n    username:  'user'\n    password:  'pass'\n    host_pool: ['host_1', ..., 'host_n']\nmetadata_store:\n    username: 'user'\n    password: 'pass'\n    host:     'active_schema_host'\n    database: 'active_schema_database'\n    zookeeper:\n        quorum: ['zk-host1', 'zk-host2']\n        path: '/path/in/zookeeper'\nhbase:\n    namespace: 'schema_namespace'\n    zookeeper_quorum:  ['hbase-zk1-host', ..., 'hbase-zkN-host']\n    # hive-imports is optional\n    hive_imports:\n        tables: ['sometable']\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        # The following are options for metrics reporters,\n        # only one should be used (graphite or console)\n        graphite:\n            namespace: 'graphite.namespace.prefix'\n            url: 'graphite_host[:\ngraphite_port (default is 3002)\n]'\n        console:\n            timeZone: UTC\n            output: stdout\n\n\n\n\nThen you can proceed with setting up the initial snapshot. By initial snapshot\nwe mean the copy of the MySQL database that we make in HBase. The reason is that\nunlike Kafka which only track changes, in HBase we want to have the entire\nhistory of database incuding the initial values. Once the inital snapshot is\ndone we can turn on the replication.\n\n\nInitial snapshot:\n\n\nInitial snapshot is the copy of mysql tables made before the replication to hbase\nis started. To make initial snapshot, two steps are performed.\n\n\nFlush the database to the binlog (using the binlog flusher tool): \nbinlog flusher\n. After the binlog-flusher finishes the flushing, by default mysql replication is stopped.\n\n\nThen, the flushed data is replicated to HBase using the replicator\nwith \ninitial-snapshot option:\n\n\njava -jar mysql-replicator.jar \\\n    --hbase-namespace $hbase-namespace \\\n    --applier hbase --schema $schema \\\n    --binlog-filename $first-binlog-filename \\\n    --config-path $config-path \\\n    --initial-snapshot\n\n\n\n\nAfter the initial snapshot has been made start the mysql replication with:\n\n\nstart slave;\n\n\n\n\nAfter this command mysql will start to write to binlogs again.\n\n\nThen start the replicator:\n\n\njava -jar mysql-replicator.jar \\\n    --hbase-namespace $hbase-namespace \\\n    --applier hbase \\\n    --schema $schema \\\n    --config-path $config-path", 
            "title": "Running the Replicator"
        }, 
        {
            "location": "/replicator/#overview", 
            "text": "Replicator reads the binary log events, augments them with schema information and writes the augmented stream to the chosen output. The following replication targets are supported: STDOUT, Kafka and HBase.   Before running the replicator,  active schema  needs to be initialized.  In addition, if you want to have the initial copy of the data (and not only the changes), you need to  flush  the contents of the database to the binlog.  Once that is done, you can proceed with configuring the replicator for the chosen replication target.", 
            "title": "Overview"
        }, 
        {
            "location": "/replicator/#stdout", 
            "text": "This option is usefull for troubleshouting and debuging. In the command line parameters we specify applier as stdout, schema which needs to be replicated, starting binlog filename and path to th config file.  java -jar mysql-replicator.jar \\\n    --applier STDOUT \\\n    --schema $schema \\\n    --binlog-filename $binlog-filename \\\n    --config-path $config-path  Where the minimal configuration file is:  replication_schema:\n    name:      'testdb'\n    username:  'test_user'\n    password:  'test_pass'\n    host_pool: ['localhost']\n\nmetadata_store:\n    username: 'meta_user'\n    password: 'meta_pass'\n    host:     'localhost'\n    database: 'testdb_active_schema'\n    file:\n        path: '/path/on/disk'\n\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        console:\n            timeZone: UTC\n            output: stdout  Where  testdb  is a schema on local mysql instance that is going to be monitored for changes and  testdb_active_schema  is a empty copy of  testdb . By empty copy we mean a copy of the schema containing tables with no data. This copy can be created by mysqldump tool. An example is given in  Setup Active Schema  section.  Example: assuming you have localhost database  testdb  and have initialized active schema  testdb_active_schema :  Create a test table:  CREATE TABLE `sometable` (\n  `pk_part_1` varchar(5) NOT NULL DEFAULT '',\n  `pk_part_2` int(11) NOT NULL DEFAULT '0',\n  `randomInt` int(11) DEFAULT NULL,\n  `randomVarchar` varchar(32) DEFAULT NULL,\n  PRIMARY KEY (`pk_part_1`,`pk_part_2`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;  Then run the replicator with:  java -jar mysql-replicator.jar \\\n    --applier STDOUT \\\n    --schema testdb \\\n    --binlog-filename $starting-binlog-file-path \\\n    --config-path $config-file-path  If you execute following sql in mysql shell:  insert into\n        sometable\n            (pk_part_1, pk_part_2, randomInt, randomVarchar)\n        values\n            ('yOeTX','3371509','911440','jfOZXWuNyJVfOzpjbsoc')  You will see the following output (here it is given with jq pretty print, but in STDOUT it is compressed json) from the replicator:  {\n   eventV4Header : {\n     timestamp : 1494843484000008,\n     eventType : 23,\n     serverId : 1,\n     eventLength : 65,\n     nextPosition : 510,\n     flags : 0,\n     timestampOfReceipt : 1494843484220\n  },\n   binlogFileName :  mysql-bin.000001 ,\n   rowBinlogEventOrdinal : 1,\n   tableName :  sometable ,\n   primaryKeyColumns : [\n     pk_part_1 ,\n     pk_part_2 \n  ],\n   rowUUID :  d36af99b-52be-4700-86d0-8c503b50658b ,\n   rowBinlogPositionID :  mysql-bin.000001:00000000000000000445:00000000000000000001 ,\n   eventColumns : {\n     pk_part_1 : {\n       type :  varchar(5) ,\n       value :  yOeTX \n    },\n     randomInt : {\n       type :  int(11) ,\n       value :  911440 \n    },\n     pk_part_2 : {\n       type :  int(11) ,\n       value :  3371509 \n    },\n     randomVarchar : {\n       type :  varchar(32) ,\n       value :  jfOZXWuNyJVfOzpjbsoc \n    }\n  },\n   eventType :  INSERT \n}", 
            "title": "Stdout"
        }, 
        {
            "location": "/replicator/#kafka", 
            "text": "You need to have a kafka topic specified in the config file.  replication_schema:\n    name:      'replicated_schema_name'\n    username:  'user'\n    password:  'pass'\n    host_pool: ['host_1', ..., 'host_n']\nmetadata_store:\n    username: 'user'\n    password: 'pass'\n    host:     'active_schema_host'\n    database: 'active_schema_database'\n    # The following are options for storing replicator metadata,\n    # only one should be used (zookeeper or file)\n    file:\n        path: '/path/on/disk'\nkafka:\n    broker:  kafka-broker-1:port,...,kafka-broken-N:port \n    topic:  topic_name\n    # tables to replicate to kafka, can be either a list of tables,\n    # or an exlcussion filter\n    tables: [ table_1 , ...,  table_N ]\n    excludetables: [ exlude_pattern_1 ,...,  exclude_pattern_N ]\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        # The following are options for metrics reporters,\n        # only one should be used (graphite or console)\n        graphite:\n            namespace: 'graphite.namespace.prefix'\n            url: 'graphite_host[: graphite_port (default is 3002) ]'\n        console:\n            timeZone: UTC\n            output: stdout  Then you start the replicator:  java -jar mysql-replicator.jar \\\n    --applier kafka \\\n    --schema $schema \\\n    --binlog-filename $binlog-filename \\\n    --config-path $config-path", 
            "title": "Kafka"
        }, 
        {
            "location": "/replicator/#hbase", 
            "text": "First step is seting up the config file, which needs to conaint the HBase zookeeper quorum  replication_schema:\n    name:      'replicated_schema_name'\n    username:  'user'\n    password:  'pass'\n    host_pool: ['host_1', ..., 'host_n']\nmetadata_store:\n    username: 'user'\n    password: 'pass'\n    host:     'active_schema_host'\n    database: 'active_schema_database'\n    zookeeper:\n        quorum: ['zk-host1', 'zk-host2']\n        path: '/path/in/zookeeper'\nhbase:\n    namespace: 'schema_namespace'\n    zookeeper_quorum:  ['hbase-zk1-host', ..., 'hbase-zkN-host']\n    # hive-imports is optional\n    hive_imports:\n        tables: ['sometable']\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        # The following are options for metrics reporters,\n        # only one should be used (graphite or console)\n        graphite:\n            namespace: 'graphite.namespace.prefix'\n            url: 'graphite_host[: graphite_port (default is 3002) ]'\n        console:\n            timeZone: UTC\n            output: stdout  Then you can proceed with setting up the initial snapshot. By initial snapshot\nwe mean the copy of the MySQL database that we make in HBase. The reason is that\nunlike Kafka which only track changes, in HBase we want to have the entire\nhistory of database incuding the initial values. Once the inital snapshot is\ndone we can turn on the replication.  Initial snapshot:  Initial snapshot is the copy of mysql tables made before the replication to hbase\nis started. To make initial snapshot, two steps are performed.  Flush the database to the binlog (using the binlog flusher tool):  binlog flusher . After the binlog-flusher finishes the flushing, by default mysql replication is stopped.  Then, the flushed data is replicated to HBase using the replicator\nwith  initial-snapshot option:  java -jar mysql-replicator.jar \\\n    --hbase-namespace $hbase-namespace \\\n    --applier hbase --schema $schema \\\n    --binlog-filename $first-binlog-filename \\\n    --config-path $config-path \\\n    --initial-snapshot  After the initial snapshot has been made start the mysql replication with:  start slave;  After this command mysql will start to write to binlogs again.  Then start the replicator:  java -jar mysql-replicator.jar \\\n    --hbase-namespace $hbase-namespace \\\n    --applier hbase \\\n    --schema $schema \\\n    --config-path $config-path", 
            "title": "HBase"
        }, 
        {
            "location": "/transactions/", 
            "text": "MySQL Transactions Support\n\n\nReplicator provides information about MySQL transaction ids to allow a reader to understand which events happened as part of the same transaction. A transaction in MySQL binlog looks like:\n\n\n - BEGIN\n - UPDATE/INSERT/DELETE/etc\n - COMMIT\n\n\n\n\nMySQL will provide a xid (transaction id) and a proper timestamp only on a COMMIT event. Timestamps of previous events represent time on an actual query invocation during transaction. Since Replicator reads binary logs as a stream it needs to buffer binlog events until a COMMIT event will be found and then override timestamps of the events. Current implementation creates a new instance of CurrentTransaction.class with a BEGIN event and fills up a buffer with following data-events. When a COMMIT event received, Replicator does timestamps override, sets the xid for the events and forwards them to an Applier.class which writes them to a destination.\n\n\nBig transactions are a special case. If there were a lot of changes in a single transaction, we can\u2019t buffer all the changes in memory. To support the big changes Replicator implements \u201crewinding\u201d of a transaction. It doesn\u2019t know if a transaction is going to be big, so it\u2019ll start with buffering as was described above. When a number of events in the transaction exceeds configured amount, Replicator will drop all of the buffered events and will read a binlog stream until a COMMIT event is reached and saved. Then it\u2019ll stop a BinlogEventProducer.class instance and reset current position to the BEGIN event. Producer will be started for the position and all the following events will be forwarded to an applier right away without buffering, but with timestamps and xids which are stored in the saved COMMIT event. When the COMMIT event reached for the second time that means we\u2019ve applied all of the transaction events and we\u2019re able to switch rewinding-mode off.\n\n\nThere might be two different types of a COMMIT event. First type is XidEvent which contains a valid xid and happens when a transaction modifies one or more XA-capable storages (for ex. InnoDB). Another type is QueryEvent which contains an sql-command \u201cCOMMIT\u201d and this type of event created when there were no XA-capable tables involved (ex. all the tables are MyISAM). For a QueryEvent event there\u2019s no xid provided by MySQL. To make that kind of events trackable on a destination data-storage and through log files there\u2019s transactionUUID provided by Replicator which is generated for each transaction during creation.\n\n\nFor the sake of internal simplicity Replicator uses the same pipeline for DDL-statements. For such events it will create a fake BEGIN event, apply an actual query and commit it with a fake COMMIT event.", 
            "title": "Transaction Support"
        }, 
        {
            "location": "/transactions/#mysql_transactions_support", 
            "text": "Replicator provides information about MySQL transaction ids to allow a reader to understand which events happened as part of the same transaction. A transaction in MySQL binlog looks like:   - BEGIN\n - UPDATE/INSERT/DELETE/etc\n - COMMIT  MySQL will provide a xid (transaction id) and a proper timestamp only on a COMMIT event. Timestamps of previous events represent time on an actual query invocation during transaction. Since Replicator reads binary logs as a stream it needs to buffer binlog events until a COMMIT event will be found and then override timestamps of the events. Current implementation creates a new instance of CurrentTransaction.class with a BEGIN event and fills up a buffer with following data-events. When a COMMIT event received, Replicator does timestamps override, sets the xid for the events and forwards them to an Applier.class which writes them to a destination.  Big transactions are a special case. If there were a lot of changes in a single transaction, we can\u2019t buffer all the changes in memory. To support the big changes Replicator implements \u201crewinding\u201d of a transaction. It doesn\u2019t know if a transaction is going to be big, so it\u2019ll start with buffering as was described above. When a number of events in the transaction exceeds configured amount, Replicator will drop all of the buffered events and will read a binlog stream until a COMMIT event is reached and saved. Then it\u2019ll stop a BinlogEventProducer.class instance and reset current position to the BEGIN event. Producer will be started for the position and all the following events will be forwarded to an applier right away without buffering, but with timestamps and xids which are stored in the saved COMMIT event. When the COMMIT event reached for the second time that means we\u2019ve applied all of the transaction events and we\u2019re able to switch rewinding-mode off.  There might be two different types of a COMMIT event. First type is XidEvent which contains a valid xid and happens when a transaction modifies one or more XA-capable storages (for ex. InnoDB). Another type is QueryEvent which contains an sql-command \u201cCOMMIT\u201d and this type of event created when there were no XA-capable tables involved (ex. all the tables are MyISAM). For a QueryEvent event there\u2019s no xid provided by MySQL. To make that kind of events trackable on a destination data-storage and through log files there\u2019s transactionUUID provided by Replicator which is generated for each transaction during creation.  For the sake of internal simplicity Replicator uses the same pipeline for DDL-statements. For such events it will create a fake BEGIN event, apply an actual query and commit it with a fake COMMIT event.", 
            "title": "MySQL Transactions Support"
        }, 
        {
            "location": "/failover_options/", 
            "text": "Overview\n\n\nThere are two types of failover that are suported:\n\n\n\n\nReplicator failover\n\n\nMySQL failover\n\n\n\n\nBoth types are based on pseudo GTIDs.\n\n\nPseudo GTIDs\n\n\nPseudo GTIDs are implemented as a \nNoOp\n statements that happen every 5 seconds on MySQL master.\nEach pseudo GTID statement goes to the binlog and contains a unique id. The statements are of the form:\n\n\ndrop view if exists `pgtid_meta`.`_pseudo_gtid_hint__asc:5B01A7FA:ECBC11E7B0CC218F:A8312C70`\n\n\n\n\nIn addition, these unique ids have a property of being ascending so given two Pseudo GTIDs we can always know which one originated before the other.\n\n\nThe script to create these events is given at \npseudo_gtid\n\n\nSafe Checkpoints\n\n\nReplicator will keep track of pseudo GTIDs and as soon as all rows that precede a given pseudoGTID are successfully committed, replicator will\nstore that pGTID as a safe checkpoint in zookeeper.\n\n\nReplicator Failover\n\n\nThere are two ways to setup replicator HA.\n\n\n\n\nDeploy the replicator as a single container in an orchestrated environments such as Marathon or Kubernetes. These orchestrators can restart it if it crushes.\n\n\nRun two instances of the replicator at the same time, where one is the leader and the other can become the leader if the first one dies.\n\n\nCombine (1) and (2) for maximum high availability.\n\n\n\n\nOnce the replicator is started/restarted it will make sure to acquire leadership lock in zookeeper and to continue replication from the last safe check point. In case multiple replicators are started only one will acquire the leadership lock and the other instances will wait until the leader dies (in which case one of them will become the new leader and continue the replication from the last safe check point)\n\n\nMySQL failover with pGTID\n\n\nIn case MySQL dies, replicator will die too, and then the standard replicator failover applies. Since the new MySQL slave will be different (with different binlog file names and positions corresponding to the same events) pGTID is used to find\nthe right binlog filename and position to continue from.\n\n\nKnown Limitations\n\n\nIn case of MySQL failover, Kafka applier can end up with up to 5 seconds of duplicate rows. Hbase does not have this limitation since the initial batch of duplicate rows will override the same versions in HBase.", 
            "title": "Failover Options"
        }, 
        {
            "location": "/failover_options/#overview", 
            "text": "There are two types of failover that are suported:   Replicator failover  MySQL failover   Both types are based on pseudo GTIDs.", 
            "title": "Overview"
        }, 
        {
            "location": "/failover_options/#pseudo_gtids", 
            "text": "Pseudo GTIDs are implemented as a  NoOp  statements that happen every 5 seconds on MySQL master.\nEach pseudo GTID statement goes to the binlog and contains a unique id. The statements are of the form:  drop view if exists `pgtid_meta`.`_pseudo_gtid_hint__asc:5B01A7FA:ECBC11E7B0CC218F:A8312C70`  In addition, these unique ids have a property of being ascending so given two Pseudo GTIDs we can always know which one originated before the other.  The script to create these events is given at  pseudo_gtid", 
            "title": "Pseudo GTIDs"
        }, 
        {
            "location": "/failover_options/#safe_checkpoints", 
            "text": "Replicator will keep track of pseudo GTIDs and as soon as all rows that precede a given pseudoGTID are successfully committed, replicator will\nstore that pGTID as a safe checkpoint in zookeeper.", 
            "title": "Safe Checkpoints"
        }, 
        {
            "location": "/failover_options/#replicator_failover", 
            "text": "There are two ways to setup replicator HA.   Deploy the replicator as a single container in an orchestrated environments such as Marathon or Kubernetes. These orchestrators can restart it if it crushes.  Run two instances of the replicator at the same time, where one is the leader and the other can become the leader if the first one dies.  Combine (1) and (2) for maximum high availability.   Once the replicator is started/restarted it will make sure to acquire leadership lock in zookeeper and to continue replication from the last safe check point. In case multiple replicators are started only one will acquire the leadership lock and the other instances will wait until the leader dies (in which case one of them will become the new leader and continue the replication from the last safe check point)", 
            "title": "Replicator Failover"
        }, 
        {
            "location": "/failover_options/#mysql_failover_with_pgtid", 
            "text": "In case MySQL dies, replicator will die too, and then the standard replicator failover applies. Since the new MySQL slave will be different (with different binlog file names and positions corresponding to the same events) pGTID is used to find\nthe right binlog filename and position to continue from.", 
            "title": "MySQL failover with pGTID"
        }, 
        {
            "location": "/failover_options/#known_limitations", 
            "text": "In case of MySQL failover, Kafka applier can end up with up to 5 seconds of duplicate rows. Hbase does not have this limitation since the initial batch of duplicate rows will override the same versions in HBase.", 
            "title": "Known Limitations"
        }, 
        {
            "location": "/snapshotter/", 
            "text": "Overview\n\n\nHBaseSnapshotter is a Spark application that takes a snapshot of an HBase table at a given point in time and stores it to a Hive table.\n\n\n\n\nCurrently there are two other solutions doing similar work, but not the exact functionality.\n\n\n\n\n\n\nHBase allows you to take a snapshot from an HBase table to another HBase table by using the provided Export and Import tools. This is done by specifying a table name, start time, end time, and number of versions, and running the export tool which will export the table to HDFS in a SequenceFile format. Then you can import the SequenceFile files to a new HBase table by using the import tool. For more information, you can check \nthis\n.\n\n\n\n\n\n\nHive storage handler allows you to use Hive queries and apply Hive operations on an HBase table. The shortcoming of this method is that it\ns able to access only the latest version of an HBase table. You can check [this] (https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration) for more information.\n\n\n\n\n\n\nHBaseSnapshotter allows you to take a snapshot from an HBase table and save it as a Hive table directly, with the possibility of selecting a desired point in time to copy the table at.\n\n\nConfiguration\n\n\nHBaseSnapshotter needs a json configuration file to be provided as a positional argument \napplication.json\n\n\nThe format of the json config should obey one the following schemas\n(\nHBaseSnapshotter.MySQLSchema\n or \nHBaseSnapshotter.HBaseSchema\n):\n\n\nHBaseSnapshotter {\n  MySQLSchema {\n    mysql.table : \"database.Tablename\"\n    mysql.schema : \"namespace:tablename\"\n    hbase.timestamp : -1\n    hbase.zookeeper_quorum : [\"hbase-zk1-host\", \"hbase-zkN-host\"]\n    hbase.table : \"namespace:tablename\"\n    hive.table : \"database.tablename\"\n  },\n  HBaseSchema {\n    hbase.timestamp : -1\n    hbase.zookeeper_quorum : [\"hbase-zk1-host\", \"hbase-zkN-host\"]\n    hbase.schema : [\"family1:qualifier1:type1\", \"familyN:qualifierN:typeN\"]\n    hbase.table : \"namespace:tablename\"\n    hive.table : \"database.tablename\"\n  }\n}\n\n\n\n\n\nmysql.table\n: original replicated MySQL table (\nstring\n)\n\n\nmysql.schema\n: schema change history table in HBase (\nstring\n)\n\n\nhbase.timestamp\n: time before which to snapshot (\nnumber\n)\n\n\nhbase.zookeeper_quorum\n: list of HBase zookeeper nodes to establish a connection with an HBase table (\nlist\n)\n\n\nhbase.table\n: replicated table in HBase (\nstring\n)\n\n\nhbase.schema\n: list of columns forming the schema of the source HBase table. A column is formatted as \nFamilyname:Qualifiername:Typename\n (\nlist\n)\n\n\nhive.table\n: destination table for snapshot in Hive (\nstring\n)\n\n\n\n\nFor snapshots from MySQL replication chains produced by the \nReplicator\n, the following is a valid configuration:\n\n\n{\n    \"mysql\": {\n        \"table\": \"database.Tablename\",\n        \"schema\": \"schema_history:database\"\n    },\n    \"hbase\": {\n        \"timestamp\": -1,\n        \"zookeeper_quorum\": [\"hbase-zk1-host\", \"hbase-zkN-host\"],\n        \"table\": \"namespace:tablename\"\n    },\n    \"hive\": {\n        \"table\": \"database.tablename\"\n    }\n}\n\n\n\nFor snapshots from arbitrary HBase tables, the following is a valid configuration:\n\n\n{\n    \"hbase\": {\n        \"timestamp\": -1,\n        \"zookeeper_quorum\": [\"hbase-zk1-host\", \"hbase-zkN-host\"],\n        \"schema\": [\"d:column_a:integer\",\n                   \"d:column_b:string\",\n                   \"d:column_c:double\"],\n        \"table\": \"namespace:tablename:\"\n    },\n    \"hive\": {\n        \"table\": \"database.tablename\"\n    }\n}\n\n\n\nTo write a configuration file, you can start by copying one of the\nexample files in the \nconf\n directory and customise it to your own\nneeds.\n\n\nHive Schema\n\n\nThe resulting Hive table will have the same schema as the source HBase\ntable (as far as it can infer the original MySQL schema, or as\ncompletely as possible given the \nhbase.schema\n list. Missing\ndatatypes will default to \nSTRING\n.  A new column will be added to the\nHive table named \nk_hbase_row_key\n for storing the HBase key of this\nrow. For MySQL snapshots, an additional row named\n\nk_replicator_row_status\n will be added to the Hive table, denoting\nwhether the row resulted from a schema change.\n\n\nUsage\n\n\nbin/hbase-snapshotter application.conf\n\n\n\nBuild\n\n\nFirst you need to build a fat jar containing all the dependencies needed by this app. Inside the project\ns folder, execute the command:\n\n\nsbt assembly\n\n\n\nIf you don\nt have sbt-assembly installed, take a look at this https://github.com/sbt/sbt-assembly. This will build a fat jar at this path:\n\n\ntarget/scala-2.10/HBaseSnapshotter-assembly-2.0.jar\n\n\n\nYou can then copy this jar along with the files hbase-snapshotter and application.json to a docker container or a hadoop box supporting Spark:\n\n\nscp target/scala-2.10/HBaseSnapshotter-assembly-2.0.jar hadoop-box.example.com:~\nscp hbase-snapshotter hadoop-box.example.com:~\nscp application.json hadoop-box.example.com:~\n\n\n\nReplace hadoop-box.example.com by the actual name of your hadoop box.\n\n\nProvide your config settings in the file \napplication.json\n.\n\n\nFinally, from the docker or hadoop box, you can run the spark app via the bash script\n\n\nbin/hbase-snapshotter application.json", 
            "title": "Hadoop/Hive Imports"
        }, 
        {
            "location": "/snapshotter/#overview", 
            "text": "HBaseSnapshotter is a Spark application that takes a snapshot of an HBase table at a given point in time and stores it to a Hive table.   Currently there are two other solutions doing similar work, but not the exact functionality.    HBase allows you to take a snapshot from an HBase table to another HBase table by using the provided Export and Import tools. This is done by specifying a table name, start time, end time, and number of versions, and running the export tool which will export the table to HDFS in a SequenceFile format. Then you can import the SequenceFile files to a new HBase table by using the import tool. For more information, you can check  this .    Hive storage handler allows you to use Hive queries and apply Hive operations on an HBase table. The shortcoming of this method is that it s able to access only the latest version of an HBase table. You can check [this] (https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration) for more information.    HBaseSnapshotter allows you to take a snapshot from an HBase table and save it as a Hive table directly, with the possibility of selecting a desired point in time to copy the table at.", 
            "title": "Overview"
        }, 
        {
            "location": "/snapshotter/#configuration", 
            "text": "HBaseSnapshotter needs a json configuration file to be provided as a positional argument  application.json  The format of the json config should obey one the following schemas\n( HBaseSnapshotter.MySQLSchema  or  HBaseSnapshotter.HBaseSchema ):  HBaseSnapshotter {\n  MySQLSchema {\n    mysql.table : \"database.Tablename\"\n    mysql.schema : \"namespace:tablename\"\n    hbase.timestamp : -1\n    hbase.zookeeper_quorum : [\"hbase-zk1-host\", \"hbase-zkN-host\"]\n    hbase.table : \"namespace:tablename\"\n    hive.table : \"database.tablename\"\n  },\n  HBaseSchema {\n    hbase.timestamp : -1\n    hbase.zookeeper_quorum : [\"hbase-zk1-host\", \"hbase-zkN-host\"]\n    hbase.schema : [\"family1:qualifier1:type1\", \"familyN:qualifierN:typeN\"]\n    hbase.table : \"namespace:tablename\"\n    hive.table : \"database.tablename\"\n  }\n}   mysql.table : original replicated MySQL table ( string )  mysql.schema : schema change history table in HBase ( string )  hbase.timestamp : time before which to snapshot ( number )  hbase.zookeeper_quorum : list of HBase zookeeper nodes to establish a connection with an HBase table ( list )  hbase.table : replicated table in HBase ( string )  hbase.schema : list of columns forming the schema of the source HBase table. A column is formatted as  Familyname:Qualifiername:Typename  ( list )  hive.table : destination table for snapshot in Hive ( string )   For snapshots from MySQL replication chains produced by the  Replicator , the following is a valid configuration:  {\n    \"mysql\": {\n        \"table\": \"database.Tablename\",\n        \"schema\": \"schema_history:database\"\n    },\n    \"hbase\": {\n        \"timestamp\": -1,\n        \"zookeeper_quorum\": [\"hbase-zk1-host\", \"hbase-zkN-host\"],\n        \"table\": \"namespace:tablename\"\n    },\n    \"hive\": {\n        \"table\": \"database.tablename\"\n    }\n}  For snapshots from arbitrary HBase tables, the following is a valid configuration:  {\n    \"hbase\": {\n        \"timestamp\": -1,\n        \"zookeeper_quorum\": [\"hbase-zk1-host\", \"hbase-zkN-host\"],\n        \"schema\": [\"d:column_a:integer\",\n                   \"d:column_b:string\",\n                   \"d:column_c:double\"],\n        \"table\": \"namespace:tablename:\"\n    },\n    \"hive\": {\n        \"table\": \"database.tablename\"\n    }\n}  To write a configuration file, you can start by copying one of the\nexample files in the  conf  directory and customise it to your own\nneeds.", 
            "title": "Configuration"
        }, 
        {
            "location": "/snapshotter/#hive_schema", 
            "text": "The resulting Hive table will have the same schema as the source HBase\ntable (as far as it can infer the original MySQL schema, or as\ncompletely as possible given the  hbase.schema  list. Missing\ndatatypes will default to  STRING .  A new column will be added to the\nHive table named  k_hbase_row_key  for storing the HBase key of this\nrow. For MySQL snapshots, an additional row named k_replicator_row_status  will be added to the Hive table, denoting\nwhether the row resulted from a schema change.", 
            "title": "Hive Schema"
        }, 
        {
            "location": "/snapshotter/#usage", 
            "text": "bin/hbase-snapshotter application.conf", 
            "title": "Usage"
        }, 
        {
            "location": "/snapshotter/#build", 
            "text": "First you need to build a fat jar containing all the dependencies needed by this app. Inside the project s folder, execute the command:  sbt assembly  If you don t have sbt-assembly installed, take a look at this https://github.com/sbt/sbt-assembly. This will build a fat jar at this path:  target/scala-2.10/HBaseSnapshotter-assembly-2.0.jar  You can then copy this jar along with the files hbase-snapshotter and application.json to a docker container or a hadoop box supporting Spark:  scp target/scala-2.10/HBaseSnapshotter-assembly-2.0.jar hadoop-box.example.com:~\nscp hbase-snapshotter hadoop-box.example.com:~\nscp application.json hadoop-box.example.com:~  Replace hadoop-box.example.com by the actual name of your hadoop box.  Provide your config settings in the file  application.json .  Finally, from the docker or hadoop box, you can run the spark app via the bash script  bin/hbase-snapshotter application.json", 
            "title": "Build"
        }, 
        {
            "location": "/validator/", 
            "text": "validator\n\n\n\n\nA service for validating replicator correctness. Documentation on the way.", 
            "title": "Setting up the Validator"
        }, 
        {
            "location": "/validator/#validator", 
            "text": "A service for validating replicator correctness. Documentation on the way.", 
            "title": "validator"
        }, 
        {
            "location": "/about/", 
            "text": "Author\n\n\nBosko Devetak\n\n\nContributors\n\n\n\n\nCarlos Tasada\n\n\nDmitrii Tcyganov\n\n\nEvgeny Dmitriev\n\n\nGreg Franklin\n\n\nIslam Hassan\n\n\nMikhail Dutikov\n\n\nMuhammad Abbady\n\n\nPhilippe Bruhat (BooK)\n\n\nPavel Salimov\n\n\nPedro Silva\n\n\nRaynald Chung\n\n\nRares Mirica\n\n\n\n\nAcknowledgment\n\n\nReplicator was originally developed for Booking.com. With approval from Booking.com, the code and specification were generalized and published as Open Source on github, for which the author would like to express his gratitude.\n\n\nCopyright and Licence\n\n\nCopyright (C) 2015, 2016, 2017 by Author and Contributors\n\n\nLicensed under the Apache License, Version 2.0 (the \nLicense\n);\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\n\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \nAS IS\n BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.", 
            "title": "About"
        }, 
        {
            "location": "/about/#author", 
            "text": "Bosko Devetak", 
            "title": "Author"
        }, 
        {
            "location": "/about/#contributors", 
            "text": "Carlos Tasada  Dmitrii Tcyganov  Evgeny Dmitriev  Greg Franklin  Islam Hassan  Mikhail Dutikov  Muhammad Abbady  Philippe Bruhat (BooK)  Pavel Salimov  Pedro Silva  Raynald Chung  Rares Mirica", 
            "title": "Contributors"
        }, 
        {
            "location": "/about/#acknowledgment", 
            "text": "Replicator was originally developed for Booking.com. With approval from Booking.com, the code and specification were generalized and published as Open Source on github, for which the author would like to express his gratitude.", 
            "title": "Acknowledgment"
        }, 
        {
            "location": "/about/#copyright_and_licence", 
            "text": "Copyright (C) 2015, 2016, 2017 by Author and Contributors  Licensed under the Apache License, Version 2.0 (the  License );\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an  AS IS  BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.", 
            "title": "Copyright and Licence"
        }
    ]
}