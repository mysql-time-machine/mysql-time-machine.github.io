{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to MySQL Time Machine\n\n\nMySQL Time Machine is a collection of services and tools for creating, processing and storing streams of MySQL data changes. Its main components are listed bellow.\n\n\n\n\nReplicator\n\n\nReplicates data changes from MySQL binlog to HBase or Kafka. In case of HBase it preserves the previous data versions. HBase version is intended for auditing of historical data. In addition can maintain special daily-changes tables which are convenient for fast and cheap imports from HBase to Hive. Kafka version is intended for processing data change streams in real time.\n\n\nBinlog Flusher\n\n\nFlushes MySQL database tables to the binlog in order to have the initial snapshot of the database in the binlog.\n\n\nHBase Snapshotter\n\n\nHBaseSnapshotter is a Spark application that takes a snapshot of a HBase table at a given point in time and stores it to a Hive table.Usually you can export from HBase to Hive but you can only get the latest version, as Hive doesn't have enough flexibility to access different versions of an HBase table. Spark framework allows this flexibility since it has the ability and the API to access and manipulate both HBase and Hive.\n\n\nValidator\n\n\nValidates the replicator correctness.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-mysql-time-machine",
            "text": "MySQL Time Machine is a collection of services and tools for creating, processing and storing streams of MySQL data changes. Its main components are listed bellow.",
            "title": "Welcome to MySQL Time Machine"
        },
        {
            "location": "/#replicator",
            "text": "Replicates data changes from MySQL binlog to HBase or Kafka. In case of HBase it preserves the previous data versions. HBase version is intended for auditing of historical data. In addition can maintain special daily-changes tables which are convenient for fast and cheap imports from HBase to Hive. Kafka version is intended for processing data change streams in real time.",
            "title": "Replicator"
        },
        {
            "location": "/#binlog-flusher",
            "text": "Flushes MySQL database tables to the binlog in order to have the initial snapshot of the database in the binlog.",
            "title": "Binlog Flusher"
        },
        {
            "location": "/#hbase-snapshotter",
            "text": "HBaseSnapshotter is a Spark application that takes a snapshot of a HBase table at a given point in time and stores it to a Hive table.Usually you can export from HBase to Hive but you can only get the latest version, as Hive doesn't have enough flexibility to access different versions of an HBase table. Spark framework allows this flexibility since it has the ability and the API to access and manipulate both HBase and Hive.",
            "title": "HBase Snapshotter"
        },
        {
            "location": "/#validator",
            "text": "Validates the replicator correctness.",
            "title": "Validator"
        },
        {
            "location": "/user-guide/replicator/",
            "text": "Replicator\n\n\nOverview\n\n\nReplicates data changes from MySQL binlog to HBase or Kafka. In case of HBase it preserves the previous data versions. HBase version is intended for auditing of historical data. In addition can maintain special daily-changes tables which are convenient for fast and cheap imports from HBase to Hive. Kafka version is intended for processing data change streams in real time.\n\n\nRunning Minimal Configuration:\n\n\nMinimal configuration needed to start replicator on localhost with STDOUT applier is:\n\n\nreplication_schema:\n    name:      'testdb'\n    username:  'test_user'\n    password:  'test_pass'\n    host_pool: ['localhost']\n\nmetadata_store:\n    username: 'meta_user'\n    password: 'meta_pass'\n    host:     'localhost'\n    database: 'testdb_active_schema'\n    file:\n        path: '/path/on/disk'\n\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        console:\n            timeZone: UTC\n            output: stdout\n\n\n\n\nWhere 'testdb' is a schema on local mysql instance that is going to be monitored for changes and 'testdb_active_schema' is a empty copy of 'testdb'. By empty copy we mean a copy of the schema containing tables with no data. This copy can be made with following steps:\n\n\ncreate new schema named 'testdb_active_schema'.\n\n\nCREATE DATABASE testdb_active_schema;\n\n\n\n\ndump the schema:\n\n\nmysqldump --host=localhost --user=test_user --password=test_pass --no-data --single-transaction testdb > schema_dump.sql\n\n\n\n\nreplace 'testdb.' with 'testdb_active_schema.' in schema_dump.sql, for example in vim:\n\n\n%s/testdb\\./testdb_active_schema\\./g\n\n\n\n\nthen import the schema to testdb_active_schema:\n\n\nmysql --host=localhost --user=meta_user --password=meta_pass < schema_dump.sql\n\n\n\n\nthen start the replicator:\n\n\njava -jar mysql-replicator.jar \\\n    --applier STDOUT \\\n    --schema testdb \\\n    --binlog-filename $starting-binlog-file-path \\\n    --config-path $config-file-path\n\n\n\n\nAdditional Configuration Options\n\n\nReplicator configuration is contained in a single YAML file. Additional options are activated by adding them to this config file. The structure of the file with all supported options is:\n\n\nreplication_schema:\n    name:      'replicated_schema_name'\n    username:  'user'\n    password:  'pass'\n    host_pool: ['host_1', ..., 'host_n']\n\nmetadata_store:\n    username: 'user'\n    password: 'pass'\n    host:     'active_schema_host'\n    database: 'active_schema_database'\n    # The following are options for storing replicator metadata,\n    # only one should be used (zookeeper or file)\n    zookeeper:\n        quorum: ['zk-host1', 'zk-host2']\n        path: '/path/in/zookeeper'\n    file:\n        path: '/path/on/disk'\n\n# Only one applier is needed (HBase or Kafka). If none is specified\n# then only STDOUT applier can be used\nhbase:\n    namespace: 'schema_namespace'\n    zookeeper_quorum:  ['hbase-zk1-host', ..., 'hbase-zkN-host']\n    # hive-imports is optional\n    hive_imports:\n        tables: ['sometable']\nkafka:\n    broker: \"kafka-broker-1:port,...,kafka-broken-N:port\"\n    topic:  topic_name\n    # tables to replicate to kafka, can be either a list of tables,\n    # or an exlcussion filter\n    tables: [\"table_1\", ..., \"table_N\"]\n    excludetables: [\"exlude_pattern_1\",..., \"exclude_pattern_N\"]\n\n# mysql-failover is optional\nmysql_failover:\n    pgtid:\n        p_gtid_pattern: $regex_pattern_to_extract_pgtid\n        p_gtid_prefix: $prefix_to_add_to_pgtid_query_used_in_orchestrator_url\n    # orchestator is optional\n    orchestrator:\n        username: orchestrator-user-name\n        password: orchestrator-password\n        url:      http://orchestrator-host/api\n\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        # The following are options for metrics reporters,\n        # only one should be used (graphite or console)\n        graphite:\n            namespace: 'graphite.namespace.prefix'\n            url: 'graphite_host[:<graphite_port (default is 3002)>]'\n        console:\n            timeZone: UTC\n            output: stdout\n\n\n\n\nReplication to Kafka\n\n\nYou need to have a kafka topic specified in the config file. Then you start the replicator:\n\n\njava -jar mysql-replicator.jar \\\n    --applier kafka \\\n    --schema $schema \\\n    --binlog-filename $binlog-filename \\\n    --config-path $config-path\n\n\n\n\nReplication to HBase\n\n\nReplication to HBase has two steps needed for setup: \n\n\n1. First the initial snapshot (hbase copy of mysql tables) needs to be made\n2. Then replication can be started\n\n\n\nInitial snapshot is the copy of mysql tables made before the replication to hbase\nis started. To make initial snapshot, two steps are performed.\n\n\nFlush the database to the binlog (using the binlog flusher tool): \nbinlog flusher\n. After the binlog-flusher finishes the flushing, by default mysql replication is stopped.\n\n\nThen, the flushed data is replicated to HBase using the replicator\nwith --initial-snapshot option:\n\n\njava -jar mysql-replicator.jar \\\n    --hbase-namespace $hbase-namespace \\\n    --applier hbase --schema $schema \\\n    --binlog-filename $first-binlog-filename \\\n    --config-path $config-path \\\n    --initial-snapshot\n\n\n\n\nAfter the initial snapshot has been made start the mysql replication with:\n\n\nstart slave;\n\n\n\n\nAfter this command mysql will start to write to binlogs again.\n\n\nThen start the replicator:\n\n\njava -jar mysql-replicator.jar \\\n    --hbase-namespace $hbase-namespace \\\n    --applier hbase \\\n    --schema $schema \\\n    --config-path $config-path\n\n\n\n\nReplicate to STDOUT\n\n\njava -jar mysql-replicator.jar \\\n    --applier STDOUT \\\n    --schema $schema \\\n    --binlog-filename $binlog-filename \\\n    --last-binlog-filename $last-binlog-filename-to-process \\\n    --config-path $config-path",
            "title": "Replicator"
        },
        {
            "location": "/user-guide/replicator/#replicator",
            "text": "",
            "title": "Replicator"
        },
        {
            "location": "/user-guide/replicator/#overview",
            "text": "Replicates data changes from MySQL binlog to HBase or Kafka. In case of HBase it preserves the previous data versions. HBase version is intended for auditing of historical data. In addition can maintain special daily-changes tables which are convenient for fast and cheap imports from HBase to Hive. Kafka version is intended for processing data change streams in real time.",
            "title": "Overview"
        },
        {
            "location": "/user-guide/replicator/#running-minimal-configuration",
            "text": "Minimal configuration needed to start replicator on localhost with STDOUT applier is:  replication_schema:\n    name:      'testdb'\n    username:  'test_user'\n    password:  'test_pass'\n    host_pool: ['localhost']\n\nmetadata_store:\n    username: 'meta_user'\n    password: 'meta_pass'\n    host:     'localhost'\n    database: 'testdb_active_schema'\n    file:\n        path: '/path/on/disk'\n\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        console:\n            timeZone: UTC\n            output: stdout  Where 'testdb' is a schema on local mysql instance that is going to be monitored for changes and 'testdb_active_schema' is a empty copy of 'testdb'. By empty copy we mean a copy of the schema containing tables with no data. This copy can be made with following steps:  create new schema named 'testdb_active_schema'.  CREATE DATABASE testdb_active_schema;  dump the schema:  mysqldump --host=localhost --user=test_user --password=test_pass --no-data --single-transaction testdb > schema_dump.sql  replace 'testdb.' with 'testdb_active_schema.' in schema_dump.sql, for example in vim:  %s/testdb\\./testdb_active_schema\\./g  then import the schema to testdb_active_schema:  mysql --host=localhost --user=meta_user --password=meta_pass < schema_dump.sql  then start the replicator:  java -jar mysql-replicator.jar \\\n    --applier STDOUT \\\n    --schema testdb \\\n    --binlog-filename $starting-binlog-file-path \\\n    --config-path $config-file-path",
            "title": "Running Minimal Configuration:"
        },
        {
            "location": "/user-guide/replicator/#additional-configuration-options",
            "text": "Replicator configuration is contained in a single YAML file. Additional options are activated by adding them to this config file. The structure of the file with all supported options is:  replication_schema:\n    name:      'replicated_schema_name'\n    username:  'user'\n    password:  'pass'\n    host_pool: ['host_1', ..., 'host_n']\n\nmetadata_store:\n    username: 'user'\n    password: 'pass'\n    host:     'active_schema_host'\n    database: 'active_schema_database'\n    # The following are options for storing replicator metadata,\n    # only one should be used (zookeeper or file)\n    zookeeper:\n        quorum: ['zk-host1', 'zk-host2']\n        path: '/path/in/zookeeper'\n    file:\n        path: '/path/on/disk'\n\n# Only one applier is needed (HBase or Kafka). If none is specified\n# then only STDOUT applier can be used\nhbase:\n    namespace: 'schema_namespace'\n    zookeeper_quorum:  ['hbase-zk1-host', ..., 'hbase-zkN-host']\n    # hive-imports is optional\n    hive_imports:\n        tables: ['sometable']\nkafka:\n    broker: \"kafka-broker-1:port,...,kafka-broken-N:port\"\n    topic:  topic_name\n    # tables to replicate to kafka, can be either a list of tables,\n    # or an exlcussion filter\n    tables: [\"table_1\", ..., \"table_N\"]\n    excludetables: [\"exlude_pattern_1\",..., \"exclude_pattern_N\"]\n\n# mysql-failover is optional\nmysql_failover:\n    pgtid:\n        p_gtid_pattern: $regex_pattern_to_extract_pgtid\n        p_gtid_prefix: $prefix_to_add_to_pgtid_query_used_in_orchestrator_url\n    # orchestator is optional\n    orchestrator:\n        username: orchestrator-user-name\n        password: orchestrator-password\n        url:      http://orchestrator-host/api\n\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        # The following are options for metrics reporters,\n        # only one should be used (graphite or console)\n        graphite:\n            namespace: 'graphite.namespace.prefix'\n            url: 'graphite_host[:<graphite_port (default is 3002)>]'\n        console:\n            timeZone: UTC\n            output: stdout",
            "title": "Additional Configuration Options"
        },
        {
            "location": "/user-guide/replicator/#replication-to-kafka",
            "text": "You need to have a kafka topic specified in the config file. Then you start the replicator:  java -jar mysql-replicator.jar \\\n    --applier kafka \\\n    --schema $schema \\\n    --binlog-filename $binlog-filename \\\n    --config-path $config-path",
            "title": "Replication to Kafka"
        },
        {
            "location": "/user-guide/replicator/#replication-to-hbase",
            "text": "Replication to HBase has two steps needed for setup:   1. First the initial snapshot (hbase copy of mysql tables) needs to be made\n2. Then replication can be started  Initial snapshot is the copy of mysql tables made before the replication to hbase\nis started. To make initial snapshot, two steps are performed.  Flush the database to the binlog (using the binlog flusher tool):  binlog flusher . After the binlog-flusher finishes the flushing, by default mysql replication is stopped.  Then, the flushed data is replicated to HBase using the replicator\nwith --initial-snapshot option:  java -jar mysql-replicator.jar \\\n    --hbase-namespace $hbase-namespace \\\n    --applier hbase --schema $schema \\\n    --binlog-filename $first-binlog-filename \\\n    --config-path $config-path \\\n    --initial-snapshot  After the initial snapshot has been made start the mysql replication with:  start slave;  After this command mysql will start to write to binlogs again.  Then start the replicator:  java -jar mysql-replicator.jar \\\n    --hbase-namespace $hbase-namespace \\\n    --applier hbase \\\n    --schema $schema \\\n    --config-path $config-path",
            "title": "Replication to HBase"
        },
        {
            "location": "/user-guide/replicator/#replicate-to-stdout",
            "text": "java -jar mysql-replicator.jar \\\n    --applier STDOUT \\\n    --schema $schema \\\n    --binlog-filename $binlog-filename \\\n    --last-binlog-filename $last-binlog-filename-to-process \\\n    --config-path $config-path",
            "title": "Replicate to STDOUT"
        },
        {
            "location": "/user-guide/snapshotter/",
            "text": "HBaseSnapshotter\n\n\nOverview\n\n\nHBaseSnapshotter is a Spark application that takes a snapshot of an HBase table at a given point in time and stores it to a Hive table. Currently there are two solutions doing similar work, but not the exact functionality.\n\n\n\n\n\n\nHBase allows you to take a snapshot from an HBase table to another HBase table by using the provided Export and Import tools. This is done by specifying a table name, start time, end time, and number of versions, and running the export tool which will export the table to HDFS in a SequenceFile format. Then you can import the SequenceFile files to a new HBase table by using the import tool. For more information, you can check \nthis\n.\n\n\n\n\n\n\nHive storage handler allows you to use Hive queries and apply Hive operations on an HBase table. The shortcoming of this method is that it's able to access only the latest version of an HBase table. You can check [this] (https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration) for more information.\n\n\n\n\n\n\nHBaseSnapshotter allows you to take a snapshot from an HBase table and save it as a Hive table directly, with the possibility of selecting a desired point in time to copy the table at.\n\n\nConfiguration\n\n\nHBaseSnapshotter needs a json configuration file to be provided as a positional argument \napplication.json\n\n\nThe format of the json config should obey one the following schemas\n(\nHBaseSnapshotter.MySQLSchema\n or \nHBaseSnapshotter.HBaseSchema\n):\n\n\nHBaseSnapshotter {\n  MySQLSchema {\n    mysql.table : \"database.Tablename\"\n    mysql.schema : \"namespace:tablename\"\n    hbase.timestamp : -1\n    hbase.zookeeper_quorum : [\"hbase-zk1-host\", \"hbase-zkN-host\"]\n    hbase.table : \"namespace:tablename\"\n    hive.table : \"database.tablename\"\n  },\n  HBaseSchema {\n    hbase.timestamp : -1\n    hbase.zookeeper_quorum : [\"hbase-zk1-host\", \"hbase-zkN-host\"]\n    hbase.schema : [\"family1:qualifier1:type1\", \"familyN:qualifierN:typeN\"]\n    hbase.table : \"namespace:tablename\"\n    hive.table : \"database.tablename\"\n  }\n}\n\n\n\n\n\nmysql.table\n: original replicated MySQL table (\nstring\n)\n\n\nmysql.schema\n: schema change history table in HBase (\nstring\n)\n\n\nhbase.timestamp\n: time before which to snapshot (\nnumber\n)\n\n\nhbase.zookeeper_quorum\n: list of HBase zookeeper nodes to establish a connection with an HBase table (\nlist\n)\n\n\nhbase.table\n: replicated table in HBase (\nstring\n)\n\n\nhbase.schema\n: list of columns forming the schema of the source HBase table. A column is formatted as \nFamilyname:Qualifiername:Typename\n (\nlist\n)\n\n\nhive.table\n: destination table for snapshot in Hive (\nstring\n)\n\n\n\n\nFor snapshots from MySQL replication chains produced by the \nReplicator\n, the following is a valid configuration:\n\n\n{\n    \"mysql\": {\n        \"table\": \"database.Tablename\",\n        \"schema\": \"schema_history:database\"\n    },\n    \"hbase\": {\n        \"timestamp\": -1,\n        \"zookeeper_quorum\": [\"hbase-zk1-host\", \"hbase-zkN-host\"],\n        \"table\": \"namespace:tablename\"\n    },\n    \"hive\": {\n        \"table\": \"database.tablename\"\n    }\n}\n\n\n\nFor snapshots from arbitrary HBase tables, the following is a valid configuration:\n\n\n{\n    \"hbase\": {\n        \"timestamp\": -1,\n        \"zookeeper_quorum\": [\"hbase-zk1-host\", \"hbase-zkN-host\"],\n        \"schema\": [\"d:column_a:integer\",\n                   \"d:column_b:string\",\n                   \"d:column_c:double\"],\n        \"table\": \"namespace:tablename:\"\n    },\n    \"hive\": {\n        \"table\": \"database.tablename\"\n    }\n}\n\n\n\nTo write a configuration file, you can start by copying one of the\nexample files in the \nconf\n directory and customise it to your own\nneeds.\n\n\nHive Schema\n\n\nThe resulting Hive table will have the same schema as the source HBase\ntable (as far as it can infer the original MySQL schema, or as\ncompletely as possible given the \nhbase.schema\n list. Missing\ndatatypes will default to \nSTRING\n.  A new column will be added to the\nHive table named \nk_hbase_row_key\n for storing the HBase key of this\nrow. For MySQL snapshots, an additional row named\n\nk_replicator_row_status\n will be added to the Hive table, denoting\nwhether the row resulted from a schema change.\n\n\nUsage\n\n\nbin/hbase-snapshotter application.conf\n\n\n\nBuild\n\n\nFirst you need to build a fat jar containing all the dependencies needed by this app. Inside the project's folder, execute the command:\n\n\nsbt assembly\n\n\n\nIf you don't have sbt-assembly installed, take a look at this https://github.com/sbt/sbt-assembly. This will build a fat jar at this path:\n\n\ntarget/scala-2.10/HBaseSnapshotter-assembly-2.0.jar\n\n\n\nYou can then copy this jar along with the files hbase-snapshotter and application.json to a docker container or a hadoop box supporting Spark:\n\n\nscp target/scala-2.10/HBaseSnapshotter-assembly-2.0.jar hadoop-box.example.com:~\nscp hbase-snapshotter hadoop-box.example.com:~\nscp application.json hadoop-box.example.com:~\n\n\n\nReplace hadoop-box.example.com by the actual name of your hadoop box.\n\n\nProvide your config settings in the file \napplication.json\n.\n\n\nFinally, from the docker or hadoop box, you can run the spark app via the bash script\n\n\nbin/hbase-snapshotter application.json",
            "title": "Snapshotter"
        },
        {
            "location": "/user-guide/snapshotter/#hbasesnapshotter",
            "text": "",
            "title": "HBaseSnapshotter"
        },
        {
            "location": "/user-guide/snapshotter/#overview",
            "text": "HBaseSnapshotter is a Spark application that takes a snapshot of an HBase table at a given point in time and stores it to a Hive table. Currently there are two solutions doing similar work, but not the exact functionality.    HBase allows you to take a snapshot from an HBase table to another HBase table by using the provided Export and Import tools. This is done by specifying a table name, start time, end time, and number of versions, and running the export tool which will export the table to HDFS in a SequenceFile format. Then you can import the SequenceFile files to a new HBase table by using the import tool. For more information, you can check  this .    Hive storage handler allows you to use Hive queries and apply Hive operations on an HBase table. The shortcoming of this method is that it's able to access only the latest version of an HBase table. You can check [this] (https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration) for more information.    HBaseSnapshotter allows you to take a snapshot from an HBase table and save it as a Hive table directly, with the possibility of selecting a desired point in time to copy the table at.",
            "title": "Overview"
        },
        {
            "location": "/user-guide/snapshotter/#configuration",
            "text": "HBaseSnapshotter needs a json configuration file to be provided as a positional argument  application.json  The format of the json config should obey one the following schemas\n( HBaseSnapshotter.MySQLSchema  or  HBaseSnapshotter.HBaseSchema ):  HBaseSnapshotter {\n  MySQLSchema {\n    mysql.table : \"database.Tablename\"\n    mysql.schema : \"namespace:tablename\"\n    hbase.timestamp : -1\n    hbase.zookeeper_quorum : [\"hbase-zk1-host\", \"hbase-zkN-host\"]\n    hbase.table : \"namespace:tablename\"\n    hive.table : \"database.tablename\"\n  },\n  HBaseSchema {\n    hbase.timestamp : -1\n    hbase.zookeeper_quorum : [\"hbase-zk1-host\", \"hbase-zkN-host\"]\n    hbase.schema : [\"family1:qualifier1:type1\", \"familyN:qualifierN:typeN\"]\n    hbase.table : \"namespace:tablename\"\n    hive.table : \"database.tablename\"\n  }\n}   mysql.table : original replicated MySQL table ( string )  mysql.schema : schema change history table in HBase ( string )  hbase.timestamp : time before which to snapshot ( number )  hbase.zookeeper_quorum : list of HBase zookeeper nodes to establish a connection with an HBase table ( list )  hbase.table : replicated table in HBase ( string )  hbase.schema : list of columns forming the schema of the source HBase table. A column is formatted as  Familyname:Qualifiername:Typename  ( list )  hive.table : destination table for snapshot in Hive ( string )   For snapshots from MySQL replication chains produced by the  Replicator , the following is a valid configuration:  {\n    \"mysql\": {\n        \"table\": \"database.Tablename\",\n        \"schema\": \"schema_history:database\"\n    },\n    \"hbase\": {\n        \"timestamp\": -1,\n        \"zookeeper_quorum\": [\"hbase-zk1-host\", \"hbase-zkN-host\"],\n        \"table\": \"namespace:tablename\"\n    },\n    \"hive\": {\n        \"table\": \"database.tablename\"\n    }\n}  For snapshots from arbitrary HBase tables, the following is a valid configuration:  {\n    \"hbase\": {\n        \"timestamp\": -1,\n        \"zookeeper_quorum\": [\"hbase-zk1-host\", \"hbase-zkN-host\"],\n        \"schema\": [\"d:column_a:integer\",\n                   \"d:column_b:string\",\n                   \"d:column_c:double\"],\n        \"table\": \"namespace:tablename:\"\n    },\n    \"hive\": {\n        \"table\": \"database.tablename\"\n    }\n}  To write a configuration file, you can start by copying one of the\nexample files in the  conf  directory and customise it to your own\nneeds.",
            "title": "Configuration"
        },
        {
            "location": "/user-guide/snapshotter/#hive-schema",
            "text": "The resulting Hive table will have the same schema as the source HBase\ntable (as far as it can infer the original MySQL schema, or as\ncompletely as possible given the  hbase.schema  list. Missing\ndatatypes will default to  STRING .  A new column will be added to the\nHive table named  k_hbase_row_key  for storing the HBase key of this\nrow. For MySQL snapshots, an additional row named k_replicator_row_status  will be added to the Hive table, denoting\nwhether the row resulted from a schema change.",
            "title": "Hive Schema"
        },
        {
            "location": "/user-guide/snapshotter/#usage",
            "text": "bin/hbase-snapshotter application.conf",
            "title": "Usage"
        },
        {
            "location": "/user-guide/snapshotter/#build",
            "text": "First you need to build a fat jar containing all the dependencies needed by this app. Inside the project's folder, execute the command:  sbt assembly  If you don't have sbt-assembly installed, take a look at this https://github.com/sbt/sbt-assembly. This will build a fat jar at this path:  target/scala-2.10/HBaseSnapshotter-assembly-2.0.jar  You can then copy this jar along with the files hbase-snapshotter and application.json to a docker container or a hadoop box supporting Spark:  scp target/scala-2.10/HBaseSnapshotter-assembly-2.0.jar hadoop-box.example.com:~\nscp hbase-snapshotter hadoop-box.example.com:~\nscp application.json hadoop-box.example.com:~  Replace hadoop-box.example.com by the actual name of your hadoop box.  Provide your config settings in the file  application.json .  Finally, from the docker or hadoop box, you can run the spark app via the bash script  bin/hbase-snapshotter application.json",
            "title": "Build"
        },
        {
            "location": "/user-guide/validator/",
            "text": "validator\n\n\nA service for validating replicator correctness.",
            "title": "Validator"
        },
        {
            "location": "/user-guide/validator/#validator",
            "text": "A service for validating replicator correctness.",
            "title": "validator"
        },
        {
            "location": "/about/",
            "text": "Author\n\n\nBosko Devetak \nbosko.devetak@gmail.com\n\n\nContributors\n\n\n\n\nGreg Franklin \ngregf1\n\n\nIslam Hassan \nishassan\n\n\nMikhail Dutikov \nmikhaildutikov\n\n\nPavel Salimov \nchcat\n\n\nPedro Silva \npedros\n\n\nRares Mirica \nmrares\n\n\nRaynald Chung \nraynald\n\n\n\n\nAcknowledgment\n\n\nReplicator was originally developed for Booking.com. With approval from Booking.com, the code and specification were generalized and published as Open Source on github, for which the author would like to express his gratitude.\n\n\nCopyright and Licence\n\n\nCopyright (C) 2015, 2016 by Bosko Devetak\n\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\n\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.",
            "title": "About"
        },
        {
            "location": "/about/#author",
            "text": "Bosko Devetak  bosko.devetak@gmail.com",
            "title": "Author"
        },
        {
            "location": "/about/#contributors",
            "text": "Greg Franklin  gregf1  Islam Hassan  ishassan  Mikhail Dutikov  mikhaildutikov  Pavel Salimov  chcat  Pedro Silva  pedros  Rares Mirica  mrares  Raynald Chung  raynald",
            "title": "Contributors"
        },
        {
            "location": "/about/#acknowledgment",
            "text": "Replicator was originally developed for Booking.com. With approval from Booking.com, the code and specification were generalized and published as Open Source on github, for which the author would like to express his gratitude.",
            "title": "Acknowledgment"
        },
        {
            "location": "/about/#copyright-and-licence",
            "text": "Copyright (C) 2015, 2016 by Bosko Devetak  Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.",
            "title": "Copyright and Licence"
        }
    ]
}