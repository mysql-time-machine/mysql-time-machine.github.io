{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to MySQL Time Machine\n\n\nMySQL Time Machine is a collection of services and tools for creating, processing and storing streams of MySQL data changes. Its main components are presented bellow.\n\n\n\nReplicator\n\n\nReplicates data changes from MySQL binlog to HBase or Kafka. In case of HBase it preserves the previous data versions. HBase version is intended for auditing of historical data. In addition can maintain special daily-changes tables which are convenient for fast and cheap imports from HBase to Hive. Kafka version is intended for processing data change streams in real time.\n\n\nThe components of the replicator are presented at the diagram bellow:\n\n\n\n\nThe numbers on the diagram mark the main tasks that the replicator does in order to reliably decode, decorate and deliver data change events to the designated target store:\n\n\n\n\n\n\nIt uses binlog client (OpenReplicator) which is decoding binary stream to a stream of java objects that can be further processed by the replicator pipeline.\n\n\n\n\n\n\nIt keeps track of schema changes that are happening and makes sure that the schema that is used to decorate events always corresponds to the position of the event in the binlog. This is important since if we were to just query the MySQL slave information_schema we do not have guarantee that the schema matches the schema of the event in the binlog. The way this is implemented is by having a shadow schema which only contains the schema and not the data. Whenever we have a DDL statement in the binlog we apply that statement to the shadow schema. We call this schema \nActive Schema\n because its state always corresponds to the\nposition of the event that was last processed, so it is always correct. The active schema is stored on a separate MySQL instance because that is a safe alternative to implementing a MySQL parser because MySQL always knows how to parse MySQL so there is no possibility of errors due to corner cases in syntax.\n\n\n\n\n\n\nIt decorates events with complete schema information even the information that\nis not present in binlog TABLE_MAP event (like if the number is unsigned). This is possible because we do not use TABLE_MAP event but instead we use the active schema from point 2.\n\n\n\n\n\n\nIt applies the schema-decorated events to the designated target. In case of HBase the applier is parallelized so ingestion rate can be very high - everything that can be pulled from MySQL can be written without replication delay.\n\n\n\n\n\n\nIt maintains safe check points in zookeeper which enable the failover mechanisms for high-availability setups, both for MySQL server failover and replicator instance failover.\n\n\n\n\n\n\nBinlog Flusher\n\n\nIn case we want to have the initial copy of the database in Kafka or HBase, before we start to replicate changes, we can achieve that by stoping the MySQL slave, flushing the database to the binlog and turning MySQL slave on again. This is the purpose of Binlog Flusher tool. It Flushes MySQL database tables to the binlog in order to have the initial snapshot of the database in the binlog.\n\n\nHBase Snapshotter\n\n\nIn case of replication to HBase, we can use the fact that HBase is temporal database which supports versioning. By default we keep 1000 versions. This means that for every field in MySQL table, on the HBase side we have all versions (with appropriate timestamps) since the replication has started. However, if we want to see how a given table looked at specific point in time, we can scan the HBase table and filter the timestamps, get the versions that correspond to desired point in time and store it in a Hive table. This is the purpose of HBaseSnapshotter.\n\n\nHBaseSnapshotter is a Spark application that takes a snapshot of a HBase table at a given point in time and stores it to a Hive table. This can not be done just by using HBase storage engine for Hive since it only works with the latest version. Spark does not have such limitations so it was used to make the snapshotter.\n\n\nValidator\n\n\nValidates the replicator correctness.", 
            "title": "Overivew"
        }, 
        {
            "location": "/#welcome_to_mysql_time_machine", 
            "text": "MySQL Time Machine is a collection of services and tools for creating, processing and storing streams of MySQL data changes. Its main components are presented bellow.", 
            "title": "Welcome to MySQL Time Machine"
        }, 
        {
            "location": "/#replicator", 
            "text": "Replicates data changes from MySQL binlog to HBase or Kafka. In case of HBase it preserves the previous data versions. HBase version is intended for auditing of historical data. In addition can maintain special daily-changes tables which are convenient for fast and cheap imports from HBase to Hive. Kafka version is intended for processing data change streams in real time.  The components of the replicator are presented at the diagram bellow:   The numbers on the diagram mark the main tasks that the replicator does in order to reliably decode, decorate and deliver data change events to the designated target store:    It uses binlog client (OpenReplicator) which is decoding binary stream to a stream of java objects that can be further processed by the replicator pipeline.    It keeps track of schema changes that are happening and makes sure that the schema that is used to decorate events always corresponds to the position of the event in the binlog. This is important since if we were to just query the MySQL slave information_schema we do not have guarantee that the schema matches the schema of the event in the binlog. The way this is implemented is by having a shadow schema which only contains the schema and not the data. Whenever we have a DDL statement in the binlog we apply that statement to the shadow schema. We call this schema  Active Schema  because its state always corresponds to the\nposition of the event that was last processed, so it is always correct. The active schema is stored on a separate MySQL instance because that is a safe alternative to implementing a MySQL parser because MySQL always knows how to parse MySQL so there is no possibility of errors due to corner cases in syntax.    It decorates events with complete schema information even the information that\nis not present in binlog TABLE_MAP event (like if the number is unsigned). This is possible because we do not use TABLE_MAP event but instead we use the active schema from point 2.    It applies the schema-decorated events to the designated target. In case of HBase the applier is parallelized so ingestion rate can be very high - everything that can be pulled from MySQL can be written without replication delay.    It maintains safe check points in zookeeper which enable the failover mechanisms for high-availability setups, both for MySQL server failover and replicator instance failover.", 
            "title": "Replicator"
        }, 
        {
            "location": "/#binlog_flusher", 
            "text": "In case we want to have the initial copy of the database in Kafka or HBase, before we start to replicate changes, we can achieve that by stoping the MySQL slave, flushing the database to the binlog and turning MySQL slave on again. This is the purpose of Binlog Flusher tool. It Flushes MySQL database tables to the binlog in order to have the initial snapshot of the database in the binlog.", 
            "title": "Binlog Flusher"
        }, 
        {
            "location": "/#hbase_snapshotter", 
            "text": "In case of replication to HBase, we can use the fact that HBase is temporal database which supports versioning. By default we keep 1000 versions. This means that for every field in MySQL table, on the HBase side we have all versions (with appropriate timestamps) since the replication has started. However, if we want to see how a given table looked at specific point in time, we can scan the HBase table and filter the timestamps, get the versions that correspond to desired point in time and store it in a Hive table. This is the purpose of HBaseSnapshotter.  HBaseSnapshotter is a Spark application that takes a snapshot of a HBase table at a given point in time and stores it to a Hive table. This can not be done just by using HBase storage engine for Hive since it only works with the latest version. Spark does not have such limitations so it was used to make the snapshotter.", 
            "title": "HBase Snapshotter"
        }, 
        {
            "location": "/#validator", 
            "text": "Validates the replicator correctness.", 
            "title": "Validator"
        }, 
        {
            "location": "/replicator/", 
            "text": "Running the Replicator\n\n\n\n\nSteps:\n\n\n\n\nFlush the database to the binlog\n\n\nSetup the active schema\n\n\nRun the replicator for specific target store\n\n\n\n\n1. Binlog Flusher\n\n\nFlushes MySQL database tables to the binlog in order to have the initial snapshot of the database in the binlog.\n\n\nUsage\n\n\nWARNING: you should NEVER run binlog-flusher on the MySQL master. Binlog flusher renames all tables during the blackhole copy and if the program does not finish successfully, the database can be left in an inconsistent state and in worst case you will need to reclone the database. The probability of this is low, but still, if it happens, you do NOT want this to happen on MySQL master.\n\n\nAssuming you adhere to the above warning, you can flush the contents of a database to the binlog with:\n\n\npython data-flusher.py --host $host\n                       [--mycnf $mycnf] \\\n                       [--db $db] [--table $table] \\\n                       [--stop-slave/--no-stop-slave] [--start-slave/--no-start-slave] \\\n                       [--skip $skip]\n\n\n\n\n\nWhere parameters are:\n\n\n\n\n--host\n: host name of the MySQL slave which databases need to be flushed to the binlog\n\n\n--mycnf\n: filename that contains the admin privileges used for the blackhole_copy of initial snapshot. Defaults to ~/my.cnf\n\n\n--db\n: comma separated list of databases to copy. Leave blank for all databases\n\n\n--table\n: comma separated list of tables to copy. Leave blank for all tables\n\n\n--stop-slave/--no-stop-slave\n: stop the replication thread whilst flushing the to the binlog. default=True\n\n\n--start-slave/--no-start-slave\n: restart the replication thread after finishing the flushing to the binlog. default=False\n\n\n--skip\n: separated list of schemas to skip (not to flush in the binlog)\n\n\n\n\nand the configuration file with admin credentials (\n--mycnf\n parameter above) has the following structure:\n\n\n[client]\nuser=admin\npassword=admin\n\n\n\n\nAfter the data has been flushed to the binlog, the replication is stopped by default (see default values for parameters above). You can start the replication with:\n\n\nmysql\n start slave;\n\n\n\n\nIn case binlog flusher didn\nt exit gracefully and the database has been left in an inconsistent state, you can run db-recovery.py to recover the database.\n\n\npython db-recovery.py --host $host \\\n                      --hashfile $hashfile \\\n                      [--mycnf .my.cnf] \\\n                      [--db $db] [--table $table] \\\n                      [--stop-slave/--no-stop-slave] \\\n                      [--start-slave/--no-start-slave] \\\n                      [--skip $skip]\n\n\n\n\nWhere \n--hashfile\n parameter contains the name of a rollback file, generated during the flush procedure, that contains the mappings from the backup table names to original table names.\n\n\n_BKTB_1, $tablename1$\n_BKTB_2, $tablename2$\n....\n\n\n\n\n2. Initializing Active Schema:\n\n\nThese are the steps to do this manually.\n\n\ncreate new schema named \ntestdb_active_schema\n.\n\n\nCREATE DATABASE testdb_active_schema;\n\n\n\n\ndump the schema:\n\n\nmysqldump --host=localhost --user=test_user --password=test_pass --no-data --single-transaction testdb \n schema_dump.sql\n\n\n\n\nreplace \ntestdb.\n with \ntestdb_active_schema.\n in schema_dump.sql, for example in vim:\n\n\n%s/testdb\\./testdb_active_schema\\./g\n\n\n\n\nthen import the schema to testdb_active_schema:\n\n\nmysql --host=localhost --user=meta_user --password=meta_pass \n schema_dump.sql\n\n\n\n\n3. Configuring replication target\n\n\nStdout\n\n\nThis option is usefull for troubleshouting and debuging. In the command line parameters we specify applier as stdout, schema which needs to be replicated, starting binlog filename and path to th config file.\n\n\njava -jar mysql-replicator.jar \\\n    --applier STDOUT \\\n    --schema $schema \\\n    --binlog-filename $binlog-filename \\\n    --config-path $config-path\n\n\n\n\nWhere the minimal configuration file is:\n\n\nreplication_schema:\n    name:      'testdb'\n    username:  'test_user'\n    password:  'test_pass'\n    host_pool: ['localhost']\n\nmetadata_store:\n    username: 'meta_user'\n    password: 'meta_pass'\n    host:     'localhost'\n    database: 'testdb_active_schema'\n    file:\n        path: '/path/on/disk'\n\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        console:\n            timeZone: UTC\n            output: stdout\n\n\n\n\nWhere \ntestdb\n is a schema on local mysql instance that is going to be monitored for changes and \ntestdb_active_schema\n is a empty copy of \ntestdb\n. By empty copy we mean a copy of the schema containing tables with no data. This copy can be created by mysqldump tool. An example is given in \nSetup Active Schema\n section.\n\n\nExample: assuming you have localhost database \ntestdb\n and have initialized active schema \ntestdb_active_schema\n:\n\n\nCreate a test table:\n\n\nCREATE TABLE `sometable` (\n  `pk_part_1` varchar(5) NOT NULL DEFAULT '',\n  `pk_part_2` int(11) NOT NULL DEFAULT '0',\n  `randomInt` int(11) DEFAULT NULL,\n  `randomVarchar` varchar(32) DEFAULT NULL,\n  PRIMARY KEY (`pk_part_1`,`pk_part_2`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;\n\n\n\n\nThen run the replicator with:\n\n\njava -jar mysql-replicator.jar \\\n    --applier STDOUT \\\n    --schema testdb \\\n    --binlog-filename $starting-binlog-file-path \\\n    --config-path $config-file-path\n\n\n\n\nIf you execute following sql in mysql shell:\n\n\ninsert into\n        sometable\n            (pk_part_1, pk_part_2, randomInt, randomVarchar)\n        values\n            ('yOeTX','3371509','911440','jfOZXWuNyJVfOzpjbsoc')\n\n\n\n\nYou will see the following output (here it is given with jq pretty print, but in STDOUT it is compressed json) from the replicator:\n\n\n{\n  \neventV4Header\n: {\n    \ntimestamp\n: 1494843484000008,\n    \neventType\n: 23,\n    \nserverId\n: 1,\n    \neventLength\n: 65,\n    \nnextPosition\n: 510,\n    \nflags\n: 0,\n    \ntimestampOfReceipt\n: 1494843484220\n  },\n  \nbinlogFileName\n: \nmysql-bin.000001\n,\n  \nrowBinlogEventOrdinal\n: 1,\n  \ntableName\n: \nsometable\n,\n  \nprimaryKeyColumns\n: [\n    \npk_part_1\n,\n    \npk_part_2\n\n  ],\n  \nrowUUID\n: \nd36af99b-52be-4700-86d0-8c503b50658b\n,\n  \nrowBinlogPositionID\n: \nmysql-bin.000001:00000000000000000445:00000000000000000001\n,\n  \neventColumns\n: {\n    \npk_part_1\n: {\n      \ntype\n: \nvarchar(5)\n,\n      \nvalue\n: \nyOeTX\n\n    },\n    \nrandomInt\n: {\n      \ntype\n: \nint(11)\n,\n      \nvalue\n: \n911440\n\n    },\n    \npk_part_2\n: {\n      \ntype\n: \nint(11)\n,\n      \nvalue\n: \n3371509\n\n    },\n    \nrandomVarchar\n: {\n      \ntype\n: \nvarchar(32)\n,\n      \nvalue\n: \njfOZXWuNyJVfOzpjbsoc\n\n    }\n  },\n  \neventType\n: \nINSERT\n\n}\n\n\n\n\nKafka\n\n\nYou need to have a kafka topic specified in the config file.\n\n\nreplication_schema:\n    name:      'replicated_schema_name'\n    username:  'user'\n    password:  'pass'\n    host_pool: ['host_1', ..., 'host_n']\nmetadata_store:\n    username: 'user'\n    password: 'pass'\n    host:     'active_schema_host'\n    database: 'active_schema_database'\n    # The following are options for storing replicator metadata,\n    # only one should be used (zookeeper or file)\n    file:\n        path: '/path/on/disk'\nkafka:\n    broker: \nkafka-broker-1:port,...,kafka-broken-N:port\n\n    topic:  topic_name\n    # tables to replicate to kafka, can be either a list of tables,\n    # or an exlcussion filter\n    tables: [\ntable_1\n, ..., \ntable_N\n]\n    excludetables: [\nexlude_pattern_1\n,..., \nexclude_pattern_N\n]\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        # The following are options for metrics reporters,\n        # only one should be used (graphite or console)\n        graphite:\n            namespace: 'graphite.namespace.prefix'\n            url: 'graphite_host[:\ngraphite_port (default is 3002)\n]'\n        console:\n            timeZone: UTC\n            output: stdout\n\n\n\n\nThen you start the replicator:\n\n\njava -jar mysql-replicator.jar \\\n    --applier kafka \\\n    --schema $schema \\\n    --binlog-filename $binlog-filename \\\n    --config-path $config-path\n\n\n\n\nHBase\n\n\nFirst step is seting up the config file, which needs to conaint the HBase zookeeper quorum\n\n\nreplication_schema:\n    name:      'replicated_schema_name'\n    username:  'user'\n    password:  'pass'\n    host_pool: ['host_1', ..., 'host_n']\nmetadata_store:\n    username: 'user'\n    password: 'pass'\n    host:     'active_schema_host'\n    database: 'active_schema_database'\n    zookeeper:\n        quorum: ['zk-host1', 'zk-host2']\n        path: '/path/in/zookeeper'\nhbase:\n    namespace: 'schema_namespace'\n    zookeeper_quorum:  ['hbase-zk1-host', ..., 'hbase-zkN-host']\n    # hive-imports is optional\n    hive_imports:\n        tables: ['sometable']\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        # The following are options for metrics reporters,\n        # only one should be used (graphite or console)\n        graphite:\n            namespace: 'graphite.namespace.prefix'\n            url: 'graphite_host[:\ngraphite_port (default is 3002)\n]'\n        console:\n            timeZone: UTC\n            output: stdout\n\n\n\n\nThen you can proceed with setting up the initial snapshot. By initial snapshot\nwe mean the copy of the MySQL database that we make in HBase. The reason is that\nunlike Kafka which only track changes, in HBase we want to have the entire\nhistory of database incuding the initial values. Once the inital snapshot is\ndone we can turn on the replication.\n\n\nInitial snapshot:\n\n\nInitial snapshot is the copy of mysql tables made before the replication to hbase\nis started. To make initial snapshot, two steps are performed.\n\n\nFlush the database to the binlog (using the binlog flusher tool): \nbinlog flusher\n. After the binlog-flusher finishes the flushing, by default mysql replication is stopped.\n\n\nThen, the flushed data is replicated to HBase using the replicator\nwith \ninitial-snapshot option:\n\n\njava -jar mysql-replicator.jar \\\n    --hbase-namespace $hbase-namespace \\\n    --applier hbase --schema $schema \\\n    --binlog-filename $first-binlog-filename \\\n    --config-path $config-path \\\n    --initial-snapshot\n\n\n\n\nAfter the initial snapshot has been made start the mysql replication with:\n\n\nstart slave;\n\n\n\n\nAfter this command mysql will start to write to binlogs again.\n\n\nThen start the replicator:\n\n\njava -jar mysql-replicator.jar \\\n    --hbase-namespace $hbase-namespace \\\n    --applier hbase \\\n    --schema $schema \\\n    --config-path $config-path", 
            "title": "Running the Replicator"
        }, 
        {
            "location": "/replicator/#running_the_replicator", 
            "text": "", 
            "title": "Running the Replicator"
        }, 
        {
            "location": "/replicator/#steps", 
            "text": "Flush the database to the binlog  Setup the active schema  Run the replicator for specific target store", 
            "title": "Steps:"
        }, 
        {
            "location": "/replicator/#1_binlog_flusher", 
            "text": "Flushes MySQL database tables to the binlog in order to have the initial snapshot of the database in the binlog.", 
            "title": "1. Binlog Flusher"
        }, 
        {
            "location": "/replicator/#usage", 
            "text": "WARNING: you should NEVER run binlog-flusher on the MySQL master. Binlog flusher renames all tables during the blackhole copy and if the program does not finish successfully, the database can be left in an inconsistent state and in worst case you will need to reclone the database. The probability of this is low, but still, if it happens, you do NOT want this to happen on MySQL master.  Assuming you adhere to the above warning, you can flush the contents of a database to the binlog with:  python data-flusher.py --host $host\n                       [--mycnf $mycnf] \\\n                       [--db $db] [--table $table] \\\n                       [--stop-slave/--no-stop-slave] [--start-slave/--no-start-slave] \\\n                       [--skip $skip]  Where parameters are:   --host : host name of the MySQL slave which databases need to be flushed to the binlog  --mycnf : filename that contains the admin privileges used for the blackhole_copy of initial snapshot. Defaults to ~/my.cnf  --db : comma separated list of databases to copy. Leave blank for all databases  --table : comma separated list of tables to copy. Leave blank for all tables  --stop-slave/--no-stop-slave : stop the replication thread whilst flushing the to the binlog. default=True  --start-slave/--no-start-slave : restart the replication thread after finishing the flushing to the binlog. default=False  --skip : separated list of schemas to skip (not to flush in the binlog)   and the configuration file with admin credentials ( --mycnf  parameter above) has the following structure:  [client]\nuser=admin\npassword=admin  After the data has been flushed to the binlog, the replication is stopped by default (see default values for parameters above). You can start the replication with:  mysql  start slave;  In case binlog flusher didn t exit gracefully and the database has been left in an inconsistent state, you can run db-recovery.py to recover the database.  python db-recovery.py --host $host \\\n                      --hashfile $hashfile \\\n                      [--mycnf .my.cnf] \\\n                      [--db $db] [--table $table] \\\n                      [--stop-slave/--no-stop-slave] \\\n                      [--start-slave/--no-start-slave] \\\n                      [--skip $skip]  Where  --hashfile  parameter contains the name of a rollback file, generated during the flush procedure, that contains the mappings from the backup table names to original table names.  _BKTB_1, $tablename1$\n_BKTB_2, $tablename2$\n....", 
            "title": "Usage"
        }, 
        {
            "location": "/replicator/#2_initializing_active_schema", 
            "text": "These are the steps to do this manually.  create new schema named  testdb_active_schema .  CREATE DATABASE testdb_active_schema;  dump the schema:  mysqldump --host=localhost --user=test_user --password=test_pass --no-data --single-transaction testdb   schema_dump.sql  replace  testdb.  with  testdb_active_schema.  in schema_dump.sql, for example in vim:  %s/testdb\\./testdb_active_schema\\./g  then import the schema to testdb_active_schema:  mysql --host=localhost --user=meta_user --password=meta_pass   schema_dump.sql", 
            "title": "2. Initializing Active Schema:"
        }, 
        {
            "location": "/replicator/#3_configuring_replication_target", 
            "text": "", 
            "title": "3. Configuring replication target"
        }, 
        {
            "location": "/replicator/#stdout", 
            "text": "This option is usefull for troubleshouting and debuging. In the command line parameters we specify applier as stdout, schema which needs to be replicated, starting binlog filename and path to th config file.  java -jar mysql-replicator.jar \\\n    --applier STDOUT \\\n    --schema $schema \\\n    --binlog-filename $binlog-filename \\\n    --config-path $config-path  Where the minimal configuration file is:  replication_schema:\n    name:      'testdb'\n    username:  'test_user'\n    password:  'test_pass'\n    host_pool: ['localhost']\n\nmetadata_store:\n    username: 'meta_user'\n    password: 'meta_pass'\n    host:     'localhost'\n    database: 'testdb_active_schema'\n    file:\n        path: '/path/on/disk'\n\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        console:\n            timeZone: UTC\n            output: stdout  Where  testdb  is a schema on local mysql instance that is going to be monitored for changes and  testdb_active_schema  is a empty copy of  testdb . By empty copy we mean a copy of the schema containing tables with no data. This copy can be created by mysqldump tool. An example is given in  Setup Active Schema  section.  Example: assuming you have localhost database  testdb  and have initialized active schema  testdb_active_schema :  Create a test table:  CREATE TABLE `sometable` (\n  `pk_part_1` varchar(5) NOT NULL DEFAULT '',\n  `pk_part_2` int(11) NOT NULL DEFAULT '0',\n  `randomInt` int(11) DEFAULT NULL,\n  `randomVarchar` varchar(32) DEFAULT NULL,\n  PRIMARY KEY (`pk_part_1`,`pk_part_2`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1;  Then run the replicator with:  java -jar mysql-replicator.jar \\\n    --applier STDOUT \\\n    --schema testdb \\\n    --binlog-filename $starting-binlog-file-path \\\n    --config-path $config-file-path  If you execute following sql in mysql shell:  insert into\n        sometable\n            (pk_part_1, pk_part_2, randomInt, randomVarchar)\n        values\n            ('yOeTX','3371509','911440','jfOZXWuNyJVfOzpjbsoc')  You will see the following output (here it is given with jq pretty print, but in STDOUT it is compressed json) from the replicator:  {\n   eventV4Header : {\n     timestamp : 1494843484000008,\n     eventType : 23,\n     serverId : 1,\n     eventLength : 65,\n     nextPosition : 510,\n     flags : 0,\n     timestampOfReceipt : 1494843484220\n  },\n   binlogFileName :  mysql-bin.000001 ,\n   rowBinlogEventOrdinal : 1,\n   tableName :  sometable ,\n   primaryKeyColumns : [\n     pk_part_1 ,\n     pk_part_2 \n  ],\n   rowUUID :  d36af99b-52be-4700-86d0-8c503b50658b ,\n   rowBinlogPositionID :  mysql-bin.000001:00000000000000000445:00000000000000000001 ,\n   eventColumns : {\n     pk_part_1 : {\n       type :  varchar(5) ,\n       value :  yOeTX \n    },\n     randomInt : {\n       type :  int(11) ,\n       value :  911440 \n    },\n     pk_part_2 : {\n       type :  int(11) ,\n       value :  3371509 \n    },\n     randomVarchar : {\n       type :  varchar(32) ,\n       value :  jfOZXWuNyJVfOzpjbsoc \n    }\n  },\n   eventType :  INSERT \n}", 
            "title": "Stdout"
        }, 
        {
            "location": "/replicator/#kafka", 
            "text": "You need to have a kafka topic specified in the config file.  replication_schema:\n    name:      'replicated_schema_name'\n    username:  'user'\n    password:  'pass'\n    host_pool: ['host_1', ..., 'host_n']\nmetadata_store:\n    username: 'user'\n    password: 'pass'\n    host:     'active_schema_host'\n    database: 'active_schema_database'\n    # The following are options for storing replicator metadata,\n    # only one should be used (zookeeper or file)\n    file:\n        path: '/path/on/disk'\nkafka:\n    broker:  kafka-broker-1:port,...,kafka-broken-N:port \n    topic:  topic_name\n    # tables to replicate to kafka, can be either a list of tables,\n    # or an exlcussion filter\n    tables: [ table_1 , ...,  table_N ]\n    excludetables: [ exlude_pattern_1 ,...,  exclude_pattern_N ]\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        # The following are options for metrics reporters,\n        # only one should be used (graphite or console)\n        graphite:\n            namespace: 'graphite.namespace.prefix'\n            url: 'graphite_host[: graphite_port (default is 3002) ]'\n        console:\n            timeZone: UTC\n            output: stdout  Then you start the replicator:  java -jar mysql-replicator.jar \\\n    --applier kafka \\\n    --schema $schema \\\n    --binlog-filename $binlog-filename \\\n    --config-path $config-path", 
            "title": "Kafka"
        }, 
        {
            "location": "/replicator/#hbase", 
            "text": "First step is seting up the config file, which needs to conaint the HBase zookeeper quorum  replication_schema:\n    name:      'replicated_schema_name'\n    username:  'user'\n    password:  'pass'\n    host_pool: ['host_1', ..., 'host_n']\nmetadata_store:\n    username: 'user'\n    password: 'pass'\n    host:     'active_schema_host'\n    database: 'active_schema_database'\n    zookeeper:\n        quorum: ['zk-host1', 'zk-host2']\n        path: '/path/in/zookeeper'\nhbase:\n    namespace: 'schema_namespace'\n    zookeeper_quorum:  ['hbase-zk1-host', ..., 'hbase-zkN-host']\n    # hive-imports is optional\n    hive_imports:\n        tables: ['sometable']\nmetrics:\n    frequency: 10 seconds\n    reporters:\n        # The following are options for metrics reporters,\n        # only one should be used (graphite or console)\n        graphite:\n            namespace: 'graphite.namespace.prefix'\n            url: 'graphite_host[: graphite_port (default is 3002) ]'\n        console:\n            timeZone: UTC\n            output: stdout  Then you can proceed with setting up the initial snapshot. By initial snapshot\nwe mean the copy of the MySQL database that we make in HBase. The reason is that\nunlike Kafka which only track changes, in HBase we want to have the entire\nhistory of database incuding the initial values. Once the inital snapshot is\ndone we can turn on the replication.  Initial snapshot:  Initial snapshot is the copy of mysql tables made before the replication to hbase\nis started. To make initial snapshot, two steps are performed.  Flush the database to the binlog (using the binlog flusher tool):  binlog flusher . After the binlog-flusher finishes the flushing, by default mysql replication is stopped.  Then, the flushed data is replicated to HBase using the replicator\nwith  initial-snapshot option:  java -jar mysql-replicator.jar \\\n    --hbase-namespace $hbase-namespace \\\n    --applier hbase --schema $schema \\\n    --binlog-filename $first-binlog-filename \\\n    --config-path $config-path \\\n    --initial-snapshot  After the initial snapshot has been made start the mysql replication with:  start slave;  After this command mysql will start to write to binlogs again.  Then start the replicator:  java -jar mysql-replicator.jar \\\n    --hbase-namespace $hbase-namespace \\\n    --applier hbase \\\n    --schema $schema \\\n    --config-path $config-path", 
            "title": "HBase"
        }, 
        {
            "location": "/snapshotter/", 
            "text": "HBaseSnapshotter\n\n\n\n\nOverview\n\n\nHBaseSnapshotter is a Spark application that takes a snapshot of an HBase table at a given point in time and stores it to a Hive table. Currently there are two solutions doing similar work, but not the exact functionality.\n\n\n\n\n\n\nHBase allows you to take a snapshot from an HBase table to another HBase table by using the provided Export and Import tools. This is done by specifying a table name, start time, end time, and number of versions, and running the export tool which will export the table to HDFS in a SequenceFile format. Then you can import the SequenceFile files to a new HBase table by using the import tool. For more information, you can check \nthis\n.\n\n\n\n\n\n\nHive storage handler allows you to use Hive queries and apply Hive operations on an HBase table. The shortcoming of this method is that it\ns able to access only the latest version of an HBase table. You can check [this] (https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration) for more information.\n\n\n\n\n\n\nHBaseSnapshotter allows you to take a snapshot from an HBase table and save it as a Hive table directly, with the possibility of selecting a desired point in time to copy the table at.\n\n\nConfiguration\n\n\nHBaseSnapshotter needs a json configuration file to be provided as a positional argument \napplication.json\n\n\nThe format of the json config should obey one the following schemas\n(\nHBaseSnapshotter.MySQLSchema\n or \nHBaseSnapshotter.HBaseSchema\n):\n\n\nHBaseSnapshotter {\n  MySQLSchema {\n    mysql.table : \"database.Tablename\"\n    mysql.schema : \"namespace:tablename\"\n    hbase.timestamp : -1\n    hbase.zookeeper_quorum : [\"hbase-zk1-host\", \"hbase-zkN-host\"]\n    hbase.table : \"namespace:tablename\"\n    hive.table : \"database.tablename\"\n  },\n  HBaseSchema {\n    hbase.timestamp : -1\n    hbase.zookeeper_quorum : [\"hbase-zk1-host\", \"hbase-zkN-host\"]\n    hbase.schema : [\"family1:qualifier1:type1\", \"familyN:qualifierN:typeN\"]\n    hbase.table : \"namespace:tablename\"\n    hive.table : \"database.tablename\"\n  }\n}\n\n\n\n\n\nmysql.table\n: original replicated MySQL table (\nstring\n)\n\n\nmysql.schema\n: schema change history table in HBase (\nstring\n)\n\n\nhbase.timestamp\n: time before which to snapshot (\nnumber\n)\n\n\nhbase.zookeeper_quorum\n: list of HBase zookeeper nodes to establish a connection with an HBase table (\nlist\n)\n\n\nhbase.table\n: replicated table in HBase (\nstring\n)\n\n\nhbase.schema\n: list of columns forming the schema of the source HBase table. A column is formatted as \nFamilyname:Qualifiername:Typename\n (\nlist\n)\n\n\nhive.table\n: destination table for snapshot in Hive (\nstring\n)\n\n\n\n\nFor snapshots from MySQL replication chains produced by the \nReplicator\n, the following is a valid configuration:\n\n\n{\n    \"mysql\": {\n        \"table\": \"database.Tablename\",\n        \"schema\": \"schema_history:database\"\n    },\n    \"hbase\": {\n        \"timestamp\": -1,\n        \"zookeeper_quorum\": [\"hbase-zk1-host\", \"hbase-zkN-host\"],\n        \"table\": \"namespace:tablename\"\n    },\n    \"hive\": {\n        \"table\": \"database.tablename\"\n    }\n}\n\n\n\nFor snapshots from arbitrary HBase tables, the following is a valid configuration:\n\n\n{\n    \"hbase\": {\n        \"timestamp\": -1,\n        \"zookeeper_quorum\": [\"hbase-zk1-host\", \"hbase-zkN-host\"],\n        \"schema\": [\"d:column_a:integer\",\n                   \"d:column_b:string\",\n                   \"d:column_c:double\"],\n        \"table\": \"namespace:tablename:\"\n    },\n    \"hive\": {\n        \"table\": \"database.tablename\"\n    }\n}\n\n\n\nTo write a configuration file, you can start by copying one of the\nexample files in the \nconf\n directory and customise it to your own\nneeds.\n\n\nHive Schema\n\n\nThe resulting Hive table will have the same schema as the source HBase\ntable (as far as it can infer the original MySQL schema, or as\ncompletely as possible given the \nhbase.schema\n list. Missing\ndatatypes will default to \nSTRING\n.  A new column will be added to the\nHive table named \nk_hbase_row_key\n for storing the HBase key of this\nrow. For MySQL snapshots, an additional row named\n\nk_replicator_row_status\n will be added to the Hive table, denoting\nwhether the row resulted from a schema change.\n\n\nUsage\n\n\nbin/hbase-snapshotter application.conf\n\n\n\nBuild\n\n\nFirst you need to build a fat jar containing all the dependencies needed by this app. Inside the project\ns folder, execute the command:\n\n\nsbt assembly\n\n\n\nIf you don\nt have sbt-assembly installed, take a look at this https://github.com/sbt/sbt-assembly. This will build a fat jar at this path:\n\n\ntarget/scala-2.10/HBaseSnapshotter-assembly-2.0.jar\n\n\n\nYou can then copy this jar along with the files hbase-snapshotter and application.json to a docker container or a hadoop box supporting Spark:\n\n\nscp target/scala-2.10/HBaseSnapshotter-assembly-2.0.jar hadoop-box.example.com:~\nscp hbase-snapshotter hadoop-box.example.com:~\nscp application.json hadoop-box.example.com:~\n\n\n\nReplace hadoop-box.example.com by the actual name of your hadoop box.\n\n\nProvide your config settings in the file \napplication.json\n.\n\n\nFinally, from the docker or hadoop box, you can run the spark app via the bash script\n\n\nbin/hbase-snapshotter application.json", 
            "title": "Hadoop/Hive Imports"
        }, 
        {
            "location": "/snapshotter/#hbasesnapshotter", 
            "text": "", 
            "title": "HBaseSnapshotter"
        }, 
        {
            "location": "/snapshotter/#overview", 
            "text": "HBaseSnapshotter is a Spark application that takes a snapshot of an HBase table at a given point in time and stores it to a Hive table. Currently there are two solutions doing similar work, but not the exact functionality.    HBase allows you to take a snapshot from an HBase table to another HBase table by using the provided Export and Import tools. This is done by specifying a table name, start time, end time, and number of versions, and running the export tool which will export the table to HDFS in a SequenceFile format. Then you can import the SequenceFile files to a new HBase table by using the import tool. For more information, you can check  this .    Hive storage handler allows you to use Hive queries and apply Hive operations on an HBase table. The shortcoming of this method is that it s able to access only the latest version of an HBase table. You can check [this] (https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration) for more information.    HBaseSnapshotter allows you to take a snapshot from an HBase table and save it as a Hive table directly, with the possibility of selecting a desired point in time to copy the table at.", 
            "title": "Overview"
        }, 
        {
            "location": "/snapshotter/#configuration", 
            "text": "HBaseSnapshotter needs a json configuration file to be provided as a positional argument  application.json  The format of the json config should obey one the following schemas\n( HBaseSnapshotter.MySQLSchema  or  HBaseSnapshotter.HBaseSchema ):  HBaseSnapshotter {\n  MySQLSchema {\n    mysql.table : \"database.Tablename\"\n    mysql.schema : \"namespace:tablename\"\n    hbase.timestamp : -1\n    hbase.zookeeper_quorum : [\"hbase-zk1-host\", \"hbase-zkN-host\"]\n    hbase.table : \"namespace:tablename\"\n    hive.table : \"database.tablename\"\n  },\n  HBaseSchema {\n    hbase.timestamp : -1\n    hbase.zookeeper_quorum : [\"hbase-zk1-host\", \"hbase-zkN-host\"]\n    hbase.schema : [\"family1:qualifier1:type1\", \"familyN:qualifierN:typeN\"]\n    hbase.table : \"namespace:tablename\"\n    hive.table : \"database.tablename\"\n  }\n}   mysql.table : original replicated MySQL table ( string )  mysql.schema : schema change history table in HBase ( string )  hbase.timestamp : time before which to snapshot ( number )  hbase.zookeeper_quorum : list of HBase zookeeper nodes to establish a connection with an HBase table ( list )  hbase.table : replicated table in HBase ( string )  hbase.schema : list of columns forming the schema of the source HBase table. A column is formatted as  Familyname:Qualifiername:Typename  ( list )  hive.table : destination table for snapshot in Hive ( string )   For snapshots from MySQL replication chains produced by the  Replicator , the following is a valid configuration:  {\n    \"mysql\": {\n        \"table\": \"database.Tablename\",\n        \"schema\": \"schema_history:database\"\n    },\n    \"hbase\": {\n        \"timestamp\": -1,\n        \"zookeeper_quorum\": [\"hbase-zk1-host\", \"hbase-zkN-host\"],\n        \"table\": \"namespace:tablename\"\n    },\n    \"hive\": {\n        \"table\": \"database.tablename\"\n    }\n}  For snapshots from arbitrary HBase tables, the following is a valid configuration:  {\n    \"hbase\": {\n        \"timestamp\": -1,\n        \"zookeeper_quorum\": [\"hbase-zk1-host\", \"hbase-zkN-host\"],\n        \"schema\": [\"d:column_a:integer\",\n                   \"d:column_b:string\",\n                   \"d:column_c:double\"],\n        \"table\": \"namespace:tablename:\"\n    },\n    \"hive\": {\n        \"table\": \"database.tablename\"\n    }\n}  To write a configuration file, you can start by copying one of the\nexample files in the  conf  directory and customise it to your own\nneeds.", 
            "title": "Configuration"
        }, 
        {
            "location": "/snapshotter/#hive_schema", 
            "text": "The resulting Hive table will have the same schema as the source HBase\ntable (as far as it can infer the original MySQL schema, or as\ncompletely as possible given the  hbase.schema  list. Missing\ndatatypes will default to  STRING .  A new column will be added to the\nHive table named  k_hbase_row_key  for storing the HBase key of this\nrow. For MySQL snapshots, an additional row named k_replicator_row_status  will be added to the Hive table, denoting\nwhether the row resulted from a schema change.", 
            "title": "Hive Schema"
        }, 
        {
            "location": "/snapshotter/#usage", 
            "text": "bin/hbase-snapshotter application.conf", 
            "title": "Usage"
        }, 
        {
            "location": "/snapshotter/#build", 
            "text": "First you need to build a fat jar containing all the dependencies needed by this app. Inside the project s folder, execute the command:  sbt assembly  If you don t have sbt-assembly installed, take a look at this https://github.com/sbt/sbt-assembly. This will build a fat jar at this path:  target/scala-2.10/HBaseSnapshotter-assembly-2.0.jar  You can then copy this jar along with the files hbase-snapshotter and application.json to a docker container or a hadoop box supporting Spark:  scp target/scala-2.10/HBaseSnapshotter-assembly-2.0.jar hadoop-box.example.com:~\nscp hbase-snapshotter hadoop-box.example.com:~\nscp application.json hadoop-box.example.com:~  Replace hadoop-box.example.com by the actual name of your hadoop box.  Provide your config settings in the file  application.json .  Finally, from the docker or hadoop box, you can run the spark app via the bash script  bin/hbase-snapshotter application.json", 
            "title": "Build"
        }, 
        {
            "location": "/validator/", 
            "text": "validator\n\n\n\n\nA service for validating replicator correctness. Documentation on the way.", 
            "title": "Setting up the Validator"
        }, 
        {
            "location": "/validator/#validator", 
            "text": "A service for validating replicator correctness. Documentation on the way.", 
            "title": "validator"
        }, 
        {
            "location": "/failover_options/", 
            "text": "Failover Options\n\n\nMySQL failover with pGTID\n\n\n    Documentation on the way.\n\n\n\n\nReplicator failover with zookeeper safe checkpoints\n\n\n    Documentation on the way.", 
            "title": "Failover Options"
        }, 
        {
            "location": "/failover_options/#failover_options", 
            "text": "", 
            "title": "Failover Options"
        }, 
        {
            "location": "/failover_options/#mysql_failover_with_pgtid", 
            "text": "Documentation on the way.", 
            "title": "MySQL failover with pGTID"
        }, 
        {
            "location": "/failover_options/#replicator_failover_with_zookeeper_safe_checkpoints", 
            "text": "Documentation on the way.", 
            "title": "Replicator failover with zookeeper safe checkpoints"
        }, 
        {
            "location": "/about/", 
            "text": "Author\n\n\nBosko Devetak\n\n\nContributors\n\n\n\n\nGreg Franklin\n\n\nIslam Hassan\n\n\nMikhail Dutikov\n\n\nPavel Salimov\n\n\nPedro Silva\n\n\nRares Mirica\n\n\nRaynald Chung\n\n\n\n\nAcknowledgment\n\n\nReplicator was originally developed for Booking.com. With approval from Booking.com, the code and specification were generalized and published as Open Source on github, for which the author would like to express his gratitude.\n\n\nCopyright and Licence\n\n\nCopyright (C) 2015, 2016, 2017 by Author and Contributors\n\n\nLicensed under the Apache License, Version 2.0 (the \nLicense\n);\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\n\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \nAS IS\n BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.", 
            "title": "About"
        }, 
        {
            "location": "/about/#author", 
            "text": "Bosko Devetak", 
            "title": "Author"
        }, 
        {
            "location": "/about/#contributors", 
            "text": "Greg Franklin  Islam Hassan  Mikhail Dutikov  Pavel Salimov  Pedro Silva  Rares Mirica  Raynald Chung", 
            "title": "Contributors"
        }, 
        {
            "location": "/about/#acknowledgment", 
            "text": "Replicator was originally developed for Booking.com. With approval from Booking.com, the code and specification were generalized and published as Open Source on github, for which the author would like to express his gratitude.", 
            "title": "Acknowledgment"
        }, 
        {
            "location": "/about/#copyright_and_licence", 
            "text": "Copyright (C) 2015, 2016, 2017 by Author and Contributors  Licensed under the Apache License, Version 2.0 (the  License );\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at  http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an  AS IS  BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.", 
            "title": "Copyright and Licence"
        }
    ]
}